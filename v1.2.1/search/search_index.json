{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"About"},{"location":"api/api-reference/","text":"API Reference \u00b6 Use the following links for the full programmatic API: Scala API docs Python API docs","title":"API Reference"},{"location":"api/api-reference/#api-reference","text":"Use the following links for the full programmatic API: Scala API docs Python API docs","title":"API Reference"},{"location":"api/creating-new-plugin/","text":"Using The Extensible API \u00b6 Intro \u00b6 Note This page explains the Extensible Data Skipping framework API. See the Concepts page for an explanation about the underlying concepts. See here for the list of currently available plugins. Xskipper supports adding your index types and specifying your own data skipping logic in order to enjoy data skipping over UDFs and a variety of data types. The pluggability is split between two main areas: Indexing Flow Interfaces for defining a new index - implementations of Index along with IndexFactory . Interface for specifying how to store the metadata in the metadatastore - implementations of MetaDataTranslator . Query Evaluation Flow Interfaces for defining how query expressions are mapped to abstract conditions on metadata - implementations of MetaDataFilter along with MetaDataFilterFactory . Interfaces for translating an abstract clause to a specific implementation according to the metadatastore - Implementations of ClauseTranslator . A new plugin can contain one or more of the above implementations. This architecture enables the registration of components using multiple packages. Using Existing Plugins \u00b6 To use a plugin, load the relevant implementations using the Registration module ( Scala , Python ). For example: Python from xskipper import Registration Registration . addMetadataFilterFactory ( spark , 'io.xskipper.plugins.regex.filter.RegexValueListMetaDataFilterFactory' ) # Add IndexFactory Registration . addIndexFactory ( spark , 'io.xskipper.plugins.regex.index.RegexIndexFactory' ) # Add MetaDataTranslator Registration . addMetaDataTranslator ( spark , 'io.xskipper.plugins.regex.parquet.RegexValueListMetaDataTranslator' ) # Add ClauseTranslator Registration . addClauseTranslator ( spark , 'io.xskipper.plugins.regex.parquet.RegexValueListClauseTranslator' ) Scala import io.xskipper._ import io.xskipper.plugins.regex.filter.RegexValueListMetaDataFilterFactory import io.xskipper.plugins.regex.index.RegexIndexFactory import io.xskipper.plugins.regex.parquet. { RegexValueListClauseTranslator , RegexValueListMetaDataTranslator } // registering the filter factories for user metadataFilters Registration . addIndexFactory ( RegexIndexFactory ) Registration . addMetadataFilterFactory ( RegexValueListMetaDataFilterFactory ) Registration . addClauseTranslator ( RegexValueListClauseTranslator ) Registration . addMetaDataTranslator ( RegexValueListMetaDataTranslator ) For the full list of plugins see here . Note When registering multiple plugins the order of registration for IndexFactory , MetadataTranslator , and ClauseTranslator matters. If two plugins define relevant translations or index creation for the same parameters the first one registered will be used. In general, you should avoid having multiple plugins that behave differently for the same indexes, metadata types or clauses. In the following sections we explain how to use the above interfaces in order to create a new plugin. The explanations will use examples from the sample plugin - xskipper-regex-plugin . The regex plugin enables indexing a text column by specifying a list of patterns and saving the matching substrings as a value list. For example, consider an application log dataset and one of its objects : application_name,log_line batch job,20/12/29 18:04:39 INFO FileSourceStrategy: Pruning directories with: batch job,20/12/29 18:04:40 INFO DAGScheduler: ResultStage 22 (collect at ParquetMetadataHandle.scala:324) finished in 0.011 s and the regex pattern \".* .* .* (.*): .*\" . When we index using the regex index, the metadata that will be saved is List(\"FileSourceStrategy\", \"DAGScheduler\") . The following query will benefit from this index and will skip the above object: SELECT * FROM tbl WHERE regexp_extract ( log_line , '.* .* .* (.*): .*' , 1 ) = 'MemoryStore' Indexing Flow \u00b6 Define the abstract metadata \u00b6 Implementation(s) of MetaDataType First you need to define the abstract metadata type that will be generated by the index. This type will hold the metadata in memory. For example, the MinMax index metadata type is a tuple of min and max values (see here ) For the Regex plugin, to store the unique list of matching substrings for a given pattern we use a HashSet of Strings (see here ). Define a new index \u00b6 Implementation(s) of Index along with IndexFactory Support for new indexes can be achieved by implementing a new class that implements the Index abstract class. A new index can use an existing MetaDataType or create its own MetaDataType along with a translation specification to the relevant metadatastore. The Index interface enables specifying one of two ways to collect the metadata: Tree Reduce - in this code path the index processes the object row by row and updates its internal state to reflect the update to the metadata. This mode enables running index creation in parallel for multiple indexes. Optimized - using this interface the index processes the entire object DataFrame and generates the metadata. For example, both the MinMaxIndex and the RegexValueListIndex . use the Tree Reduce mode to accumlate the list of unique matches. Along with the Index you need to define an IndexFactory - the IndexFactory specifies how to recreate the index instance when loading the index parameters from the metadatastore. For example, see RegexIndexFactory . Define translation for the metadata \u00b6 Implementation(s) of MetaDataTranslator Info Xskipper uses by default Parquet as the metadatastore. The Parquet metadatastore stores the metadata for the objects as rows in parquet files (for more details see here ). The API enables defining your own metadatastore. Here we focus on storing the metadata in the Parquet metadatastore. Therefore, the translations we cover here relate to the Parquet metadatastore. In order to store the abstract metadata defined above in the metadata store we have to specify a suitable translation which will map it to a valid representation for the metadatastore. For the Parquet metadatastore we have two options: Convert the metadata to an internal Spark Row which will later be saved to Parquet automatically, as Spark supports Parquet out of the box. For example, the MinMax index translates its values to a nested row with min and max values (see here ) Use a UDT to save the serialized abstract metadata. In some cases translating the metadata to Spark Row is not possible, therefore we save the metadata as a serialized binary. To do so you have 2 options: Use the default java serialization provided by the parquet metadata store. To do so you need to define the UDT and register it. For example, for bloom filter, we use the following definition to get the default java serialization: class BloomFilterMetaDataTypeUDT extends MetadataTypeUDT [ BloomFilterMetaData ] Then register the UDT to Spark using the ParquetMetadataStoreUDTRegistration object: ParquetMetadataStoreUDTRegistration . registerUDT ( classOf [ BloomFilterMetaData ]. getName , classOf [ BloomFilterMetaDataTypeUDT ]. getName ) Note that the bloom filter has the above definition built in so there is no need to register it. Note The UDT must be defined and registered in any program that uses the index. A recommended pattern is to define and register the UDT in the Clause Translator object where you also define the Clause Translation logic. This object will be loaded when registering the Clause Translator. Define your own UDT with custom serialization logic - similar to the above only this time you implement your own UDT. See the MetadataTypeUDT class for a reference. For the regex plugin we use the first option and translate the list of values to an array of values for storing in Parquet format (see here ). Query Evaluation Flow \u00b6 Define the abstract clause \u00b6 Implementations of Clause First, you need to define the abstract clause that will be created by the Filter. The Clause specifices an abstract condition which was deduced from the query and should operate on the metadata in order to determine the relevant objects. Each Clause is then translated to an explicit implementation according to the metadatastore type. For example, for the MinMax index we define a MinMaxClause which follows the logic that was presented here . For the Regex Plugin we use a Clause which holds the required matching patterns from the query (see here ). Define a new filter \u00b6 Implementation(s) of MetaDataFilter along with MetaDataFilterFactory The filter processes the query tree and labels it with clauses. In most cases we would like to map expressions to clauses. Therefore, xskipper provides a basic implementation of a filter called BaseMetadataFilter which processes the query tree automatically for AND and OR operators, leaving the user to handle only the remaining expressions. Implementations which extend the BaseMetadataFilter need only specify how expressions are mapped to clauses. For example, RegexValueListFilter and MinMaxFilter map the query expressions according to the logic presented here . A more advanced filter can process the entire tree by implementing the MetaDataFilter class without using the BaseMetadataFilter . Along with a Filter you need to define a MetadataFilterFactory . The MetadataFilterFactory specifies which filters should run given the available indexes. For example, see the RegexIndexFactory . Define translation for the clause \u00b6 Implementations of ClauseTranslator Info Xskipper uses Parquet as the metadatastore by default. The Parquet metadatastore stores the metadata for the objects as rows in parquet files (for more details see here ). Spark is used as the engine to run the abstract clauses on the metadata. The API enables defining your own metadatastore. Here we focus on the metadata in the Parquet metadatastore. Therefore, the translations are relevant to the Parquet metadatastore. In order to process a clause, the abstract clause defined above needs to be translated to a form that is executable by the metadatastore. For the Parquet metadatastore we have 2 options: Translate the Clause to a native Spark operation - this is useful when you have a built-in expression in Spark that can process the metadata. For example, for the MinMax index we use Spark\u2019s built-in inequality operators (>, <, >=, <=) to translate the abstract clause (see here ). Use a UDF that will process the metadata - this is useful when the metadata is saved by serializing the abstract metadata type or when there is no built-in operation that implements the logic needed in order to process the metadata. For example, for the BloomFilter index which we serializes its metadata, we use a UDF to check whether the given value exists in the metadata or not (see here ). For the Regex Plugin we translate the clause to use Spark's arrays_overlap and array_except functions in order to check if the values in the clause exist in the metadata (see here ).","title":"Using The Extensible API"},{"location":"api/creating-new-plugin/#using-the-extensible-api","text":"","title":"Using The Extensible API"},{"location":"api/creating-new-plugin/#intro","text":"Note This page explains the Extensible Data Skipping framework API. See the Concepts page for an explanation about the underlying concepts. See here for the list of currently available plugins. Xskipper supports adding your index types and specifying your own data skipping logic in order to enjoy data skipping over UDFs and a variety of data types. The pluggability is split between two main areas: Indexing Flow Interfaces for defining a new index - implementations of Index along with IndexFactory . Interface for specifying how to store the metadata in the metadatastore - implementations of MetaDataTranslator . Query Evaluation Flow Interfaces for defining how query expressions are mapped to abstract conditions on metadata - implementations of MetaDataFilter along with MetaDataFilterFactory . Interfaces for translating an abstract clause to a specific implementation according to the metadatastore - Implementations of ClauseTranslator . A new plugin can contain one or more of the above implementations. This architecture enables the registration of components using multiple packages.","title":"Intro"},{"location":"api/creating-new-plugin/#using-existing-plugins","text":"To use a plugin, load the relevant implementations using the Registration module ( Scala , Python ). For example: Python from xskipper import Registration Registration . addMetadataFilterFactory ( spark , 'io.xskipper.plugins.regex.filter.RegexValueListMetaDataFilterFactory' ) # Add IndexFactory Registration . addIndexFactory ( spark , 'io.xskipper.plugins.regex.index.RegexIndexFactory' ) # Add MetaDataTranslator Registration . addMetaDataTranslator ( spark , 'io.xskipper.plugins.regex.parquet.RegexValueListMetaDataTranslator' ) # Add ClauseTranslator Registration . addClauseTranslator ( spark , 'io.xskipper.plugins.regex.parquet.RegexValueListClauseTranslator' ) Scala import io.xskipper._ import io.xskipper.plugins.regex.filter.RegexValueListMetaDataFilterFactory import io.xskipper.plugins.regex.index.RegexIndexFactory import io.xskipper.plugins.regex.parquet. { RegexValueListClauseTranslator , RegexValueListMetaDataTranslator } // registering the filter factories for user metadataFilters Registration . addIndexFactory ( RegexIndexFactory ) Registration . addMetadataFilterFactory ( RegexValueListMetaDataFilterFactory ) Registration . addClauseTranslator ( RegexValueListClauseTranslator ) Registration . addMetaDataTranslator ( RegexValueListMetaDataTranslator ) For the full list of plugins see here . Note When registering multiple plugins the order of registration for IndexFactory , MetadataTranslator , and ClauseTranslator matters. If two plugins define relevant translations or index creation for the same parameters the first one registered will be used. In general, you should avoid having multiple plugins that behave differently for the same indexes, metadata types or clauses. In the following sections we explain how to use the above interfaces in order to create a new plugin. The explanations will use examples from the sample plugin - xskipper-regex-plugin . The regex plugin enables indexing a text column by specifying a list of patterns and saving the matching substrings as a value list. For example, consider an application log dataset and one of its objects : application_name,log_line batch job,20/12/29 18:04:39 INFO FileSourceStrategy: Pruning directories with: batch job,20/12/29 18:04:40 INFO DAGScheduler: ResultStage 22 (collect at ParquetMetadataHandle.scala:324) finished in 0.011 s and the regex pattern \".* .* .* (.*): .*\" . When we index using the regex index, the metadata that will be saved is List(\"FileSourceStrategy\", \"DAGScheduler\") . The following query will benefit from this index and will skip the above object: SELECT * FROM tbl WHERE regexp_extract ( log_line , '.* .* .* (.*): .*' , 1 ) = 'MemoryStore'","title":"Using Existing Plugins"},{"location":"api/creating-new-plugin/#indexing-flow","text":"","title":"Indexing Flow"},{"location":"api/creating-new-plugin/#define-the-abstract-metadata","text":"Implementation(s) of MetaDataType First you need to define the abstract metadata type that will be generated by the index. This type will hold the metadata in memory. For example, the MinMax index metadata type is a tuple of min and max values (see here ) For the Regex plugin, to store the unique list of matching substrings for a given pattern we use a HashSet of Strings (see here ).","title":"Define the abstract metadata"},{"location":"api/creating-new-plugin/#define-a-new-index","text":"Implementation(s) of Index along with IndexFactory Support for new indexes can be achieved by implementing a new class that implements the Index abstract class. A new index can use an existing MetaDataType or create its own MetaDataType along with a translation specification to the relevant metadatastore. The Index interface enables specifying one of two ways to collect the metadata: Tree Reduce - in this code path the index processes the object row by row and updates its internal state to reflect the update to the metadata. This mode enables running index creation in parallel for multiple indexes. Optimized - using this interface the index processes the entire object DataFrame and generates the metadata. For example, both the MinMaxIndex and the RegexValueListIndex . use the Tree Reduce mode to accumlate the list of unique matches. Along with the Index you need to define an IndexFactory - the IndexFactory specifies how to recreate the index instance when loading the index parameters from the metadatastore. For example, see RegexIndexFactory .","title":"Define a new index"},{"location":"api/creating-new-plugin/#define-translation-for-the-metadata","text":"Implementation(s) of MetaDataTranslator Info Xskipper uses by default Parquet as the metadatastore. The Parquet metadatastore stores the metadata for the objects as rows in parquet files (for more details see here ). The API enables defining your own metadatastore. Here we focus on storing the metadata in the Parquet metadatastore. Therefore, the translations we cover here relate to the Parquet metadatastore. In order to store the abstract metadata defined above in the metadata store we have to specify a suitable translation which will map it to a valid representation for the metadatastore. For the Parquet metadatastore we have two options: Convert the metadata to an internal Spark Row which will later be saved to Parquet automatically, as Spark supports Parquet out of the box. For example, the MinMax index translates its values to a nested row with min and max values (see here ) Use a UDT to save the serialized abstract metadata. In some cases translating the metadata to Spark Row is not possible, therefore we save the metadata as a serialized binary. To do so you have 2 options: Use the default java serialization provided by the parquet metadata store. To do so you need to define the UDT and register it. For example, for bloom filter, we use the following definition to get the default java serialization: class BloomFilterMetaDataTypeUDT extends MetadataTypeUDT [ BloomFilterMetaData ] Then register the UDT to Spark using the ParquetMetadataStoreUDTRegistration object: ParquetMetadataStoreUDTRegistration . registerUDT ( classOf [ BloomFilterMetaData ]. getName , classOf [ BloomFilterMetaDataTypeUDT ]. getName ) Note that the bloom filter has the above definition built in so there is no need to register it. Note The UDT must be defined and registered in any program that uses the index. A recommended pattern is to define and register the UDT in the Clause Translator object where you also define the Clause Translation logic. This object will be loaded when registering the Clause Translator. Define your own UDT with custom serialization logic - similar to the above only this time you implement your own UDT. See the MetadataTypeUDT class for a reference. For the regex plugin we use the first option and translate the list of values to an array of values for storing in Parquet format (see here ).","title":"Define translation for the metadata"},{"location":"api/creating-new-plugin/#query-evaluation-flow","text":"","title":"Query Evaluation Flow"},{"location":"api/creating-new-plugin/#define-the-abstract-clause","text":"Implementations of Clause First, you need to define the abstract clause that will be created by the Filter. The Clause specifices an abstract condition which was deduced from the query and should operate on the metadata in order to determine the relevant objects. Each Clause is then translated to an explicit implementation according to the metadatastore type. For example, for the MinMax index we define a MinMaxClause which follows the logic that was presented here . For the Regex Plugin we use a Clause which holds the required matching patterns from the query (see here ).","title":"Define the abstract clause"},{"location":"api/creating-new-plugin/#define-a-new-filter","text":"Implementation(s) of MetaDataFilter along with MetaDataFilterFactory The filter processes the query tree and labels it with clauses. In most cases we would like to map expressions to clauses. Therefore, xskipper provides a basic implementation of a filter called BaseMetadataFilter which processes the query tree automatically for AND and OR operators, leaving the user to handle only the remaining expressions. Implementations which extend the BaseMetadataFilter need only specify how expressions are mapped to clauses. For example, RegexValueListFilter and MinMaxFilter map the query expressions according to the logic presented here . A more advanced filter can process the entire tree by implementing the MetaDataFilter class without using the BaseMetadataFilter . Along with a Filter you need to define a MetadataFilterFactory . The MetadataFilterFactory specifies which filters should run given the available indexes. For example, see the RegexIndexFactory .","title":"Define a new filter"},{"location":"api/creating-new-plugin/#define-translation-for-the-clause","text":"Implementations of ClauseTranslator Info Xskipper uses Parquet as the metadatastore by default. The Parquet metadatastore stores the metadata for the objects as rows in parquet files (for more details see here ). Spark is used as the engine to run the abstract clauses on the metadata. The API enables defining your own metadatastore. Here we focus on the metadata in the Parquet metadatastore. Therefore, the translations are relevant to the Parquet metadatastore. In order to process a clause, the abstract clause defined above needs to be translated to a form that is executable by the metadatastore. For the Parquet metadatastore we have 2 options: Translate the Clause to a native Spark operation - this is useful when you have a built-in expression in Spark that can process the metadata. For example, for the MinMax index we use Spark\u2019s built-in inequality operators (>, <, >=, <=) to translate the abstract clause (see here ). Use a UDF that will process the metadata - this is useful when the metadata is saved by serializing the abstract metadata type or when there is no built-in operation that implements the logic needed in order to process the metadata. For example, for the BloomFilter index which we serializes its metadata, we use a UDF to check whether the given value exists in the metadata or not (see here ). For the Regex Plugin we translate the clause to use Spark's arrays_overlap and array_except functions in order to check if the values in the clause exist in the metadata (see here ).","title":"Define translation for the clause"},{"location":"api/indexing/","text":"Indexing \u00b6 For every column in the object, Xskipper can collect summary metadata. This metadata is used during query evaluation to skip over objects which have no relevant data. Default Indexes \u00b6 The following indexes are supported out of the box: Index type Description Applicable to predicates in WHERE clauses Column types MinMax Stores minimum and maximum values for a column <,<=,=,>=,> All types except for complex types. See Supported Spark SQL data types . ValueList Stores the list of unique values for the column =,IN,LIKE All types except for complex types. See Supported Spark SQL data types . BloomFilter Uses a bloom filter to test for set membership =,IN Byte, String, Long, Integer, Short In order to add an index using the IndexBuilder to specify the required indexes, for example: Python # create Xskipper instance for the sample dataset xskipper = Xskipper ( spark , dataset_location ) # remove index if exists if xskipper . isIndexed (): xskipper . dropIndex () xskipper . indexBuilder () \\ . addMinMaxIndex ( \"temp\" ) \\ . addValueListIndex ( \"city\" ) \\ . addBloomFilterIndex ( \"vid\" ) \\ . build ( reader ) \\ . show ( 10 , False ) Scala // create Xskipper instance for the sample dataset val xskipper = new Xskipper ( spark , dataset_location ) // remove existing index if needed if ( xskipper . isIndexed ()) { xskipper . dropIndex () } xskipper . indexBuilder () . addMinMaxIndex ( \"temp\" ) . addValueListIndex ( \"city\" ) . addBloomFilterIndex ( \"vid\" ) . build ( reader ) . show ( false ) By default, the indexes are stored as parquet files stored in storage Each parquet file with row per each object in the dataset. For more information about the parquet metadatastore see here . Plugins \u00b6 Xskipper supports adding new indexes using a pluggable system . For instructions on how to create a new plugin see here . Supported plugins \u00b6 Currently the following plugins are supported (in addition to the built-in indexes: MinMax, ValueList and BloomFilter): Regex Plugin - An index which enables to save a value list for a given regex. Setting up a plugin \u00b6 In order to use a plugin you first need to register the needed classes. For example, for the Regex Plugin : Python from xskipper import Registration Registration . addMetadataFilterFactory ( spark , 'io.xskipper.plugins.regex.filter.RegexValueListMetaDataFilterFactory' ) # Add IndexFactory Registration . addIndexFactory ( spark , 'io.xskipper.plugins.regex.index.RegexIndexFactory' ) # Add MetaDataTranslator Registration . addMetaDataTranslator ( spark , 'io.xskipper.plugins.regex.parquet.RegexValueListMetaDataTranslator' ) # Add ClauseTranslator Registration . addClauseTranslator ( spark , 'io.xskipper.plugins.regex.parquet.RegexValueListClauseTranslator' ) Scala import io.xskipper._ import io.xskipper.plugins.regex.filter.RegexValueListMetaDataFilterFactory import io.xskipper.plugins.regex.index.RegexIndexFactory import io.xskipper.plugins.regex.parquet. { RegexValueListClauseTranslator , RegexValueListMetaDataTranslator } // registering the filter factories for user metadataFilters Registration . addIndexFactory ( RegexIndexFactory ) Registration . addMetadataFilterFactory ( RegexValueListMetaDataFilterFactory ) Registration . addClauseTranslator ( RegexValueListClauseTranslator ) Registration . addMetaDataTranslator ( RegexValueListMetaDataTranslator ) Index building \u00b6 In order to build an index you can use the addCustomIndex API. For example for the Regex Plugin : Python xskipper = Xskipper ( spark , dataset_path ) # adding the index using the custom index API xskipper . indexBuilder () \\ . addCustomIndex ( \"io.xskipper.plugins.regex.index.RegexValueListIndex\" , [ \"log_line\" ], { \"io.xskipper.plugins.regex.pattern.r0\" : \".* .* .* (.*): .*\" }) \\ . build ( reader ) \\ . show ( 10 , False ) Scala import io.xskipper.plugins.regex.implicits._ // index the dataset val xskipper = new Xskipper ( spark , dataset_path ) xskipper . indexBuilder () // using the implicit method defined in the plugin implicits . addRegexValueListIndex ( \"log_line\" , Seq ( \".* .* .* (.*): .*\" )) // equivalent //.addCustomIndex(RegexValueListIndex(\"log_line\", Seq(\".* .* .* (.*): .*\"))) . build ( reader ). show ( false ) Creating you own plugin \u00b6 In order to create your own plugin see here .","title":"Indexing"},{"location":"api/indexing/#indexing","text":"For every column in the object, Xskipper can collect summary metadata. This metadata is used during query evaluation to skip over objects which have no relevant data.","title":"Indexing"},{"location":"api/indexing/#default-indexes","text":"The following indexes are supported out of the box: Index type Description Applicable to predicates in WHERE clauses Column types MinMax Stores minimum and maximum values for a column <,<=,=,>=,> All types except for complex types. See Supported Spark SQL data types . ValueList Stores the list of unique values for the column =,IN,LIKE All types except for complex types. See Supported Spark SQL data types . BloomFilter Uses a bloom filter to test for set membership =,IN Byte, String, Long, Integer, Short In order to add an index using the IndexBuilder to specify the required indexes, for example: Python # create Xskipper instance for the sample dataset xskipper = Xskipper ( spark , dataset_location ) # remove index if exists if xskipper . isIndexed (): xskipper . dropIndex () xskipper . indexBuilder () \\ . addMinMaxIndex ( \"temp\" ) \\ . addValueListIndex ( \"city\" ) \\ . addBloomFilterIndex ( \"vid\" ) \\ . build ( reader ) \\ . show ( 10 , False ) Scala // create Xskipper instance for the sample dataset val xskipper = new Xskipper ( spark , dataset_location ) // remove existing index if needed if ( xskipper . isIndexed ()) { xskipper . dropIndex () } xskipper . indexBuilder () . addMinMaxIndex ( \"temp\" ) . addValueListIndex ( \"city\" ) . addBloomFilterIndex ( \"vid\" ) . build ( reader ) . show ( false ) By default, the indexes are stored as parquet files stored in storage Each parquet file with row per each object in the dataset. For more information about the parquet metadatastore see here .","title":"Default Indexes"},{"location":"api/indexing/#plugins","text":"Xskipper supports adding new indexes using a pluggable system . For instructions on how to create a new plugin see here .","title":"Plugins"},{"location":"api/indexing/#supported-plugins","text":"Currently the following plugins are supported (in addition to the built-in indexes: MinMax, ValueList and BloomFilter): Regex Plugin - An index which enables to save a value list for a given regex.","title":"Supported plugins"},{"location":"api/indexing/#setting-up-a-plugin","text":"In order to use a plugin you first need to register the needed classes. For example, for the Regex Plugin : Python from xskipper import Registration Registration . addMetadataFilterFactory ( spark , 'io.xskipper.plugins.regex.filter.RegexValueListMetaDataFilterFactory' ) # Add IndexFactory Registration . addIndexFactory ( spark , 'io.xskipper.plugins.regex.index.RegexIndexFactory' ) # Add MetaDataTranslator Registration . addMetaDataTranslator ( spark , 'io.xskipper.plugins.regex.parquet.RegexValueListMetaDataTranslator' ) # Add ClauseTranslator Registration . addClauseTranslator ( spark , 'io.xskipper.plugins.regex.parquet.RegexValueListClauseTranslator' ) Scala import io.xskipper._ import io.xskipper.plugins.regex.filter.RegexValueListMetaDataFilterFactory import io.xskipper.plugins.regex.index.RegexIndexFactory import io.xskipper.plugins.regex.parquet. { RegexValueListClauseTranslator , RegexValueListMetaDataTranslator } // registering the filter factories for user metadataFilters Registration . addIndexFactory ( RegexIndexFactory ) Registration . addMetadataFilterFactory ( RegexValueListMetaDataFilterFactory ) Registration . addClauseTranslator ( RegexValueListClauseTranslator ) Registration . addMetaDataTranslator ( RegexValueListMetaDataTranslator )","title":"Setting up a plugin"},{"location":"api/indexing/#index-building","text":"In order to build an index you can use the addCustomIndex API. For example for the Regex Plugin : Python xskipper = Xskipper ( spark , dataset_path ) # adding the index using the custom index API xskipper . indexBuilder () \\ . addCustomIndex ( \"io.xskipper.plugins.regex.index.RegexValueListIndex\" , [ \"log_line\" ], { \"io.xskipper.plugins.regex.pattern.r0\" : \".* .* .* (.*): .*\" }) \\ . build ( reader ) \\ . show ( 10 , False ) Scala import io.xskipper.plugins.regex.implicits._ // index the dataset val xskipper = new Xskipper ( spark , dataset_path ) xskipper . indexBuilder () // using the implicit method defined in the plugin implicits . addRegexValueListIndex ( \"log_line\" , Seq ( \".* .* .* (.*): .*\" )) // equivalent //.addCustomIndex(RegexValueListIndex(\"log_line\", Seq(\".* .* .* (.*): .*\"))) . build ( reader ). show ( false )","title":"Index building"},{"location":"api/indexing/#creating-you-own-plugin","text":"In order to create your own plugin see here .","title":"Creating you own plugin"},{"location":"api/configuration/configuration/","text":"Configuration \u00b6 Xskipper enables to configure the instances using JVM wide configtation which applies for all created Xskipper instances. In addition one can set specific configuration for a given instance as described below. For the configuration related to the parquet metadatastore see here . Setting JVM wide configuration \u00b6 Use the following to set JVM wide configuration properties for Xskipper instances. These configurations will be applied by default on each new instance. Python from xskipper import Xskipper # TODO: fill with config values conf = dict () Xskipper . setConf ( spark , conf ) Scala import io.xskipper._ // TODO: fill with config values val conf = Map () Xskipper . setConf ( conf ) Setting configuration for a specific Xskipper instance \u00b6 Use the following to set the configuration for a specific Xskipper instance Python # call set params to make sure it overwrites JVM wide config # TODO: fill with config values params = dict () xskipper . setParams ( params ) Scala val xskipper = new Xskipper ( spark , dataset_location ) // call set params to make sure it overwrites JVM wide config // TODO: fill with config values val params = Map () xskipper . setParams ( params ) Xskipper properties \u00b6 Property Default Description io.xskipper.evaluation.enabled false When true, queries will run in evaluation mode. When running in evaluation mode all of the indexed dataset will only be processed for skipping stats and no data will be read. The evaluation mode is useful when we want to inspect the skipping stats io.xskipper.timeout 10 The timeout in minutes to retrieve the indexed objects and the relevant objects for the query from the metadatastore io.xskipper.identifierclass io.xskipper.utils. identifier.Identifier The fully qualified name of an identifier class to be loaded using reflection used to specify how the table identifier and file ID are determined. For more information see here . io.xskipper.index.parallelism 10 defines the number of concurrent objects that will be indexed io.xskipper.index.minchunksize 1 Defines the minimum chunk size to be used when indexing. The chunk size will be multiplied by 2 till reaching the metadataStore upload chunk size io.xskipper.index.bloom.fpp 0.01 Indicates the bloom filter default fpp io.xskipper.index.bloom.ndv 100000 Indicates the bloom filter expected number of distinct values io.xskipper.index.minmax .readoptimized.parquet true Indicates whether the collection of min/max stats for Numeric columns when working with Parquet objects will be done in an optimized way by reading the stats from the Parquet footer io.xskipper.index.minmax .readoptimized.parquet.parallelism 10000 The number of objects to be indexed in parallel when having only minmax indexes on parquet objects io.xskipper.index.minmax .inFilterThreshold 100 defines number of values in an IN filter above we will push down only one min/max condition based on the min and maximum of the entire list on parquet objects io.xskipper.index.memoryFraction 0.2 The memory fraction from the driver memory that indexing will use in order to determine the maximum chunk size dynamically Identifier Class \u00b6 Each dataset/table/file that that is indexed by Xskipper is identified by some Identifier. This identifier is used for in order to determine the following: The file ID for each indexed file - typically the file ID is comprised of the file name concatenated with the last modified time stamp to detect changes since last indexing time. Display name for identifiers Disply name for paths Currently Xskipper supports two implementation of the Identifer class: Default Identifier The File ID is - <file_name>#<last_modification_time> The Identifier is the uri after removing a trailing / if exists. The Path and Display name are the given paths IBM COS Identifier - differs from the default identifier by The identifier for IBM COS paths when using Stocator is removing the service part and keeps the bucket name. So, for example the identifier for cos://mybucket.service/path/to/dataset/ is cos://mybucket/path/to/dataset For most cases you can use the default implementation, however if you require some custom logic you can add your own implementation for the Identifier class (for example, the IBM COS Identifier as an example). The Identifier class is by setting the paramater io.xskipper.identifierclass to the and determines the logic for inferring the identifier for each","title":"Xskipper Configuration"},{"location":"api/configuration/configuration/#configuration","text":"Xskipper enables to configure the instances using JVM wide configtation which applies for all created Xskipper instances. In addition one can set specific configuration for a given instance as described below. For the configuration related to the parquet metadatastore see here .","title":"Configuration"},{"location":"api/configuration/configuration/#setting-jvm-wide-configuration","text":"Use the following to set JVM wide configuration properties for Xskipper instances. These configurations will be applied by default on each new instance. Python from xskipper import Xskipper # TODO: fill with config values conf = dict () Xskipper . setConf ( spark , conf ) Scala import io.xskipper._ // TODO: fill with config values val conf = Map () Xskipper . setConf ( conf )","title":"Setting JVM wide configuration"},{"location":"api/configuration/configuration/#setting-configuration-for-a-specific-xskipper-instance","text":"Use the following to set the configuration for a specific Xskipper instance Python # call set params to make sure it overwrites JVM wide config # TODO: fill with config values params = dict () xskipper . setParams ( params ) Scala val xskipper = new Xskipper ( spark , dataset_location ) // call set params to make sure it overwrites JVM wide config // TODO: fill with config values val params = Map () xskipper . setParams ( params )","title":"Setting configuration for a specific Xskipper instance"},{"location":"api/configuration/configuration/#xskipper-properties","text":"Property Default Description io.xskipper.evaluation.enabled false When true, queries will run in evaluation mode. When running in evaluation mode all of the indexed dataset will only be processed for skipping stats and no data will be read. The evaluation mode is useful when we want to inspect the skipping stats io.xskipper.timeout 10 The timeout in minutes to retrieve the indexed objects and the relevant objects for the query from the metadatastore io.xskipper.identifierclass io.xskipper.utils. identifier.Identifier The fully qualified name of an identifier class to be loaded using reflection used to specify how the table identifier and file ID are determined. For more information see here . io.xskipper.index.parallelism 10 defines the number of concurrent objects that will be indexed io.xskipper.index.minchunksize 1 Defines the minimum chunk size to be used when indexing. The chunk size will be multiplied by 2 till reaching the metadataStore upload chunk size io.xskipper.index.bloom.fpp 0.01 Indicates the bloom filter default fpp io.xskipper.index.bloom.ndv 100000 Indicates the bloom filter expected number of distinct values io.xskipper.index.minmax .readoptimized.parquet true Indicates whether the collection of min/max stats for Numeric columns when working with Parquet objects will be done in an optimized way by reading the stats from the Parquet footer io.xskipper.index.minmax .readoptimized.parquet.parallelism 10000 The number of objects to be indexed in parallel when having only minmax indexes on parquet objects io.xskipper.index.minmax .inFilterThreshold 100 defines number of values in an IN filter above we will push down only one min/max condition based on the min and maximum of the entire list on parquet objects io.xskipper.index.memoryFraction 0.2 The memory fraction from the driver memory that indexing will use in order to determine the maximum chunk size dynamically","title":"Xskipper properties"},{"location":"api/configuration/configuration/#identifier-class","text":"Each dataset/table/file that that is indexed by Xskipper is identified by some Identifier. This identifier is used for in order to determine the following: The file ID for each indexed file - typically the file ID is comprised of the file name concatenated with the last modified time stamp to detect changes since last indexing time. Display name for identifiers Disply name for paths Currently Xskipper supports two implementation of the Identifer class: Default Identifier The File ID is - <file_name>#<last_modification_time> The Identifier is the uri after removing a trailing / if exists. The Path and Display name are the given paths IBM COS Identifier - differs from the default identifier by The identifier for IBM COS paths when using Stocator is removing the service part and keeps the bucket name. So, for example the identifier for cos://mybucket.service/path/to/dataset/ is cos://mybucket/path/to/dataset For most cases you can use the default implementation, however if you require some custom logic you can add your own implementation for the Identifier class (for example, the IBM COS Identifier as an example). The Identifier class is by setting the paramater io.xskipper.identifierclass to the and determines the logic for inferring the identifier for each","title":"Identifier Class"},{"location":"api/configuration/parquet-mdstore-configuration/","text":"Parquet Metadatastore properties \u00b6 Xskipper uses Parquet as the metadatastore. The Parquet metadatastore store the metadata for the objects as rows in parquet file. See here for more details. The following are parameters relevant for the Parquet metadatastore. These parameters can be set as: JVM wide configuration as described here . Specific Xskipper instance as described here . Property Default Description io.xskipper.parquet .mdlocation N/A The metadata location according to the type io.xskipper.parquet .mdlocation.type EXPLICIT_BASE_PATH_LOCATION See here io.xskipper.parquet .index.chunksize 25000 The number of objects to index in each chunk io.xskipper.parquet .maxRecordsPerMetadataFile 50000 The number of records per metadata file used in the compact stage to limit the number of records for small sized metadata files. io.xskipper.parquet .maxMetadataFileSize 33554432 (32MB) The expected max size of each metadata file will be used by the compact function to distribute the data to multiple files to distribute the data to multiple files in such way that each is less than the expected max size of each metadata file used in conjunction with the max records per file configuration io.xskipper.parquet .encryption.plaintext.footer false Whether or not to use plain footer io.xskipper.parquet .encryption.footer.key N/A The encryption key that will be used to encrypt the metadata footer Types of metadata location \u00b6 The parquet metadatastore looks up the metadata location according to the paramater io.xskipper.parquet.mdlocation , this parameter is interpreted according to the URL type defined in the parameter io.xskipper.parquet.mdlocation.type . The following options are available for the parameter io.xskipper.parquet.mdlocation.type : EXPLICIT_BASE_PATH_LOCATION : This is the default. An explicit definition of the base path to the metadata, which is combined with a data set identifier. This case can be used to configure the Xskipper JVM wide settings and have all of data sets metadata saved under the base path. EXPLICIT_LOCATION : An explicit full path to the metadata. HIVE_TABLE_NAME : The name of the Hive table (in the form of <db>.<table> ) that contains the exact path of the metadata in the table properties under the parameter io.xskipper.parquet.mdlocation . HIVE_DB_NAME : The name of the Hive database that contains the base path of the metadata in the database properties under the parameter io.xskipper.parquet.mdlocation . For more information on how the metadata path is resolved see here . You should set the io.xskipper.parquet.mdlocation in one of two ways: Setting base location for metadata (recommended) \u00b6 This configuration is useful for setting base location once for all datasets and should be used with the EXPLICIT_BASE_PATH_LOCATION or HIVE_DB_NAME types. The location of the metadata for each data set will be inferred automatically by combining the base path with a data set identifier. For example, setting EXPLICIT_BASE_PATH_LOCATION : Python from xskipper import Xskipper # The base location to store all indexes # TODO: change to your index base location md_base_location = \"/tmp/metadata\" # Configuring the JVM wide parameters conf = dict ([ ( \"io.xskipper.parquet.mdlocation\" , md_base_location ), ( \"io.xskipper.parquet.mdlocation.type\" , \"EXPLICIT_BASE_PATH_LOCATION\" )]) Xskipper . setConf ( spark , conf ) Scala from xskipper import Xskipper # The base location to store all indexes # TODO : change to your index base location md_base_location = \"/tmp/metadata\" # Configuring the JVM wide parameters conf = dict ([ ( \" io.xskipper.parquet.mdlocation \" , md_base_location ) , ( \" io.xskipper.parquet.mdlocation. type \" , \" EXPLICIT_BASE_PATH_LOCATION \" )]) Xskipper . setConf ( spark , conf ) Setting an explicit metadata location for Xskipper instance \u00b6 This configuration is useful for setting a specific metadata location for a certain data set and should be used with the EXPLICIT_LOCATION or HIVE_TABLE_NAME type. You can also set it to EXPLICIT_BASE_PATH_LOCATION or HIVE_DB_NAME if you want to override the JVM defaults. For example setting an EXPLICIT_LOCATION for xskipper instance: Python xskipper = Xskipper ( spark_session , dataset_location ) # call set params to make sure it overwrites JVM wide config params = dict ([ ( 'io.xskipper.parquet.mdlocation' , '<your metadata>' ), ( 'io.xskipper.parquet.mdlocation.type' , 'EXPLICIT_BASE_PATH_LOCATION' )]) xskipper . setParams ( params ) Scala val xskipper = new Xskipper ( spark , dataset_location ) // call set params to make sure it overwrites JVM wide config val params = Map ( \"io.xskipper.parquet.mdlocation\" -> \"<your metadata>\" , \"io.xskipper.parquet.mdlocation.type\" -> \"EXPLICIT_BASE_PATH_LOCATION\" ) xskipper . setParams ( params ) Note that when setting the type to HIVE_TABLE_NAME you should first set the parameter io.xskipper.parquet.mdlocation in the table properties to point to the metadata location. You can the set the metadata location for hive table manually using the following DDL: ALTER TABLE myDb . myTable SET TBLPROPERTIES ( 'io.xskipper.parquet.mdlocation' = '/location/of/metadata' ) If the location does not exist in the Hive table, xskipper will try to look up the parameter io.xskipper.parquet.mdlocation in the table's database and treat it as a base path location. When using HIVE_TABLE_NAME as the location type once indexing is done the parameter io.xskipper.parquet.mdlocation is set in the table properties. (in case a fallback to the database base path occured then the resolved path from using the database metadata location as base path is stored in the table properties) Metadata path resolving \u00b6 The following explains how the metadata path is resolved: During Indexing the metadata location is inferred according to the parameters parameters io.xskipper.parquet.mdlocation and io.xskipper.parquet.mdlocation.type which are set on the xskipper instance. These parameters are the default JVM wide parameters unless set differently for the given instance. During query run time, the metadata location is inferred according to the following If there is an active xskipper instance in the JVM for the dataset/table use it's metadata configuration. Otherwise: For Datasets: Look up the parameters io.xskipper.parquet.mdlocation and io.xskipper.parquet.mdlocation.type in the JVM wide configuration in order to infer the metadata location. For Hive tables (with partitions 1 ): If the table contains the parameter io.xskipper.parquet.mdlocation use it as the metadata location. If not, look up the parameter io.xskipper.parquet.mdlocation in the table's database and treat it as a base path location. To index tables without partitions, index the physical location directly. \u21a9","title":"Parquet Metadatastore Configuration"},{"location":"api/configuration/parquet-mdstore-configuration/#parquet-metadatastore-properties","text":"Xskipper uses Parquet as the metadatastore. The Parquet metadatastore store the metadata for the objects as rows in parquet file. See here for more details. The following are parameters relevant for the Parquet metadatastore. These parameters can be set as: JVM wide configuration as described here . Specific Xskipper instance as described here . Property Default Description io.xskipper.parquet .mdlocation N/A The metadata location according to the type io.xskipper.parquet .mdlocation.type EXPLICIT_BASE_PATH_LOCATION See here io.xskipper.parquet .index.chunksize 25000 The number of objects to index in each chunk io.xskipper.parquet .maxRecordsPerMetadataFile 50000 The number of records per metadata file used in the compact stage to limit the number of records for small sized metadata files. io.xskipper.parquet .maxMetadataFileSize 33554432 (32MB) The expected max size of each metadata file will be used by the compact function to distribute the data to multiple files to distribute the data to multiple files in such way that each is less than the expected max size of each metadata file used in conjunction with the max records per file configuration io.xskipper.parquet .encryption.plaintext.footer false Whether or not to use plain footer io.xskipper.parquet .encryption.footer.key N/A The encryption key that will be used to encrypt the metadata footer","title":"Parquet Metadatastore properties"},{"location":"api/configuration/parquet-mdstore-configuration/#types-of-metadata-location","text":"The parquet metadatastore looks up the metadata location according to the paramater io.xskipper.parquet.mdlocation , this parameter is interpreted according to the URL type defined in the parameter io.xskipper.parquet.mdlocation.type . The following options are available for the parameter io.xskipper.parquet.mdlocation.type : EXPLICIT_BASE_PATH_LOCATION : This is the default. An explicit definition of the base path to the metadata, which is combined with a data set identifier. This case can be used to configure the Xskipper JVM wide settings and have all of data sets metadata saved under the base path. EXPLICIT_LOCATION : An explicit full path to the metadata. HIVE_TABLE_NAME : The name of the Hive table (in the form of <db>.<table> ) that contains the exact path of the metadata in the table properties under the parameter io.xskipper.parquet.mdlocation . HIVE_DB_NAME : The name of the Hive database that contains the base path of the metadata in the database properties under the parameter io.xskipper.parquet.mdlocation . For more information on how the metadata path is resolved see here . You should set the io.xskipper.parquet.mdlocation in one of two ways:","title":"Types of metadata location"},{"location":"api/configuration/parquet-mdstore-configuration/#setting-base-location-for-metadata-recommended","text":"This configuration is useful for setting base location once for all datasets and should be used with the EXPLICIT_BASE_PATH_LOCATION or HIVE_DB_NAME types. The location of the metadata for each data set will be inferred automatically by combining the base path with a data set identifier. For example, setting EXPLICIT_BASE_PATH_LOCATION : Python from xskipper import Xskipper # The base location to store all indexes # TODO: change to your index base location md_base_location = \"/tmp/metadata\" # Configuring the JVM wide parameters conf = dict ([ ( \"io.xskipper.parquet.mdlocation\" , md_base_location ), ( \"io.xskipper.parquet.mdlocation.type\" , \"EXPLICIT_BASE_PATH_LOCATION\" )]) Xskipper . setConf ( spark , conf ) Scala from xskipper import Xskipper # The base location to store all indexes # TODO : change to your index base location md_base_location = \"/tmp/metadata\" # Configuring the JVM wide parameters conf = dict ([ ( \" io.xskipper.parquet.mdlocation \" , md_base_location ) , ( \" io.xskipper.parquet.mdlocation. type \" , \" EXPLICIT_BASE_PATH_LOCATION \" )]) Xskipper . setConf ( spark , conf )","title":"Setting base location for metadata (recommended)"},{"location":"api/configuration/parquet-mdstore-configuration/#setting-an-explicit-metadata-location-for-xskipper-instance","text":"This configuration is useful for setting a specific metadata location for a certain data set and should be used with the EXPLICIT_LOCATION or HIVE_TABLE_NAME type. You can also set it to EXPLICIT_BASE_PATH_LOCATION or HIVE_DB_NAME if you want to override the JVM defaults. For example setting an EXPLICIT_LOCATION for xskipper instance: Python xskipper = Xskipper ( spark_session , dataset_location ) # call set params to make sure it overwrites JVM wide config params = dict ([ ( 'io.xskipper.parquet.mdlocation' , '<your metadata>' ), ( 'io.xskipper.parquet.mdlocation.type' , 'EXPLICIT_BASE_PATH_LOCATION' )]) xskipper . setParams ( params ) Scala val xskipper = new Xskipper ( spark , dataset_location ) // call set params to make sure it overwrites JVM wide config val params = Map ( \"io.xskipper.parquet.mdlocation\" -> \"<your metadata>\" , \"io.xskipper.parquet.mdlocation.type\" -> \"EXPLICIT_BASE_PATH_LOCATION\" ) xskipper . setParams ( params ) Note that when setting the type to HIVE_TABLE_NAME you should first set the parameter io.xskipper.parquet.mdlocation in the table properties to point to the metadata location. You can the set the metadata location for hive table manually using the following DDL: ALTER TABLE myDb . myTable SET TBLPROPERTIES ( 'io.xskipper.parquet.mdlocation' = '/location/of/metadata' ) If the location does not exist in the Hive table, xskipper will try to look up the parameter io.xskipper.parquet.mdlocation in the table's database and treat it as a base path location. When using HIVE_TABLE_NAME as the location type once indexing is done the parameter io.xskipper.parquet.mdlocation is set in the table properties. (in case a fallback to the database base path occured then the resolved path from using the database metadata location as base path is stored in the table properties)","title":"Setting an explicit metadata location for Xskipper instance"},{"location":"api/configuration/parquet-mdstore-configuration/#metadata-path-resolving","text":"The following explains how the metadata path is resolved: During Indexing the metadata location is inferred according to the parameters parameters io.xskipper.parquet.mdlocation and io.xskipper.parquet.mdlocation.type which are set on the xskipper instance. These parameters are the default JVM wide parameters unless set differently for the given instance. During query run time, the metadata location is inferred according to the following If there is an active xskipper instance in the JVM for the dataset/table use it's metadata configuration. Otherwise: For Datasets: Look up the parameters io.xskipper.parquet.mdlocation and io.xskipper.parquet.mdlocation.type in the JVM wide configuration in order to infer the metadata location. For Hive tables (with partitions 1 ): If the table contains the parameter io.xskipper.parquet.mdlocation use it as the metadata location. If not, look up the parameter io.xskipper.parquet.mdlocation in the table's database and treat it as a base path location. To index tables without partitions, index the physical location directly. \u21a9","title":"Metadata path resolving"},{"location":"api/developer/build/","text":"Building from source \u00b6 Xskipper is compiled using SBT . To compile, run build/sbt compile To generate artifacts, run build/sbt package To execute tests, run build/sbt test Refer to SBT docs for more commands.","title":"Build"},{"location":"api/developer/build/#building-from-source","text":"Xskipper is compiled using SBT . To compile, run build/sbt compile To generate artifacts, run build/sbt package To execute tests, run build/sbt test Refer to SBT docs for more commands.","title":"Building from source"},{"location":"api/developer/generate-doc/","text":"API Doc Generation \u00b6 The following contains instructions on how to generate the Scala and Python docs for Xskipper. Xskipper Python Docs \u00b6 Xskipper Python docs are generated using Sphinx Installation \u00b6 pip install -U Sphinx pip install mock Local Changes \u00b6 The doc configs are location in python/doc . Make the needed changes to the python files and then create the HTML files using: cd python/doc make html The results will be located under _build/html Note By default the version that will be used is the version that is written in version.sbt If you would like to change the version change it in the config file conf.py Publishing \u00b6 To update the docs copy _build/html to site/docs/api/pythondoc/<version_number> . For example: cp -r _build/html ../../site/docs/api/pythondoc/1.2.0 Then update the index file to point to the new API documentation. Xskipper Scala Docs \u00b6 Xskipper Scala docs are generated using sbt doc Generate the docs \u00b6 Run the following: sbt clean compile package sbt doc Publishing \u00b6 To update the docs copy ../../../../target/scala-2.12/api/ to ../scaladoc/1.2.0/ (assuming you shell is pointed at this folder). For example: cp -r ../../../../target/scala-2.12/api/ ../scaladoc/1.2.0/ Then update the index file to point to the new API documentation.","title":"API Doc Generation"},{"location":"api/developer/generate-doc/#api-doc-generation","text":"The following contains instructions on how to generate the Scala and Python docs for Xskipper.","title":"API Doc Generation"},{"location":"api/developer/generate-doc/#xskipper-python-docs","text":"Xskipper Python docs are generated using Sphinx","title":"Xskipper Python Docs"},{"location":"api/developer/generate-doc/#installation","text":"pip install -U Sphinx pip install mock","title":"Installation"},{"location":"api/developer/generate-doc/#local-changes","text":"The doc configs are location in python/doc . Make the needed changes to the python files and then create the HTML files using: cd python/doc make html The results will be located under _build/html Note By default the version that will be used is the version that is written in version.sbt If you would like to change the version change it in the config file conf.py","title":"Local Changes"},{"location":"api/developer/generate-doc/#publishing","text":"To update the docs copy _build/html to site/docs/api/pythondoc/<version_number> . For example: cp -r _build/html ../../site/docs/api/pythondoc/1.2.0 Then update the index file to point to the new API documentation.","title":"Publishing"},{"location":"api/developer/generate-doc/#xskipper-scala-docs","text":"Xskipper Scala docs are generated using sbt doc","title":"Xskipper Scala Docs"},{"location":"api/developer/generate-doc/#generate-the-docs","text":"Run the following: sbt clean compile package sbt doc","title":"Generate the docs"},{"location":"api/developer/generate-doc/#publishing_1","text":"To update the docs copy ../../../../target/scala-2.12/api/ to ../scaladoc/1.2.0/ (assuming you shell is pointed at this folder). For example: cp -r ../../../../target/scala-2.12/api/ ../scaladoc/1.2.0/ Then update the index file to point to the new API documentation.","title":"Publishing"},{"location":"api/developer/metadata-versioning/","text":"Metadata Versioning \u00b6 The versioning mechanism, version numbers and inter-version support matrix is up to the metadata store implementation. However, some details were pulled above the metadata store abstraction. A metadata store must be able to report the metadata version status , which is defined as follows: object MetadataVersionStatus extends Enumeration { type MetadataVersionStatus = Value /** * The stored metadata is from exactly the same version * as the metadata version of this jar. */ val CURRENT = Value ( \"current\" ) /** * The stored metadata is from a version strictly smaller than * the metadata version of this jar. however, it can be used for filtering * and a REFRESH operation will result in the metadata having the `CURRENT` status */ val DEPRECATED_SUPPORTED = Value ( \"deprecated_supported\" ) /** * The stored metadata is from a version strictly smaller than * the metadata version of this jar, a version so old it can't be used for filtering. * a REFRESH operation may or may not be able to upgrade it, up to the * metadata store's `isMetadataUpgradePossible` method. */ val DEPRECATED_UNSUPPORTED = Value ( \"deprecated_unsupported\" ) /** * The stored metadata is from a version which is strictly greater * than the metadata version of this jar. * the metadata store is not expected to able to either read or refresh this metadata. */ val TOO_NEW = Value ( \"too_new\" ) } and so the metadata store must be able to perform the following operation regarding versioning: Report the Metadata Version Status: def getMdVersionStatus () : MetadataVersionStatus Report whether or not a metadata upgrade is possible: def isMetadataUpgradePossible () : Boolean Perform a Metadata Upgrade if reported as possible: def upgradeMetadata () : Unit During a REFRESH operation, the metadata version status is first checked, and upgraded if necessary, before performing the actual refresh or removing metadata for deleted objects. Any failure during this process (e.g., metadata version status TOO_NEW , isMetadataUpgradePossible returning false etc.) will fail the refresh.","title":"Metadata versioning"},{"location":"api/developer/metadata-versioning/#metadata-versioning","text":"The versioning mechanism, version numbers and inter-version support matrix is up to the metadata store implementation. However, some details were pulled above the metadata store abstraction. A metadata store must be able to report the metadata version status , which is defined as follows: object MetadataVersionStatus extends Enumeration { type MetadataVersionStatus = Value /** * The stored metadata is from exactly the same version * as the metadata version of this jar. */ val CURRENT = Value ( \"current\" ) /** * The stored metadata is from a version strictly smaller than * the metadata version of this jar. however, it can be used for filtering * and a REFRESH operation will result in the metadata having the `CURRENT` status */ val DEPRECATED_SUPPORTED = Value ( \"deprecated_supported\" ) /** * The stored metadata is from a version strictly smaller than * the metadata version of this jar, a version so old it can't be used for filtering. * a REFRESH operation may or may not be able to upgrade it, up to the * metadata store's `isMetadataUpgradePossible` method. */ val DEPRECATED_UNSUPPORTED = Value ( \"deprecated_unsupported\" ) /** * The stored metadata is from a version which is strictly greater * than the metadata version of this jar. * the metadata store is not expected to able to either read or refresh this metadata. */ val TOO_NEW = Value ( \"too_new\" ) } and so the metadata store must be able to perform the following operation regarding versioning: Report the Metadata Version Status: def getMdVersionStatus () : MetadataVersionStatus Report whether or not a metadata upgrade is possible: def isMetadataUpgradePossible () : Boolean Perform a Metadata Upgrade if reported as possible: def upgradeMetadata () : Unit During a REFRESH operation, the metadata version status is first checked, and upgraded if necessary, before performing the actual refresh or removing metadata for deleted objects. Any failure during this process (e.g., metadata version status TOO_NEW , isMetadataUpgradePossible returning false etc.) will fail the refresh.","title":"Metadata Versioning"},{"location":"api/developer/parquet-metadatastore-spec/","text":"Parquet Metadatastore Spec \u00b6 This is a specification for how metadata is represented in the Parquet Metadata store. We support multiple versions and document them here. Note that the version number is stored in metadata files in a specific column of their Spark schema. Behind the scenes it is saved in the KV Metadata of the resulting Parquet files. This means that the version number can never imply metadata file locations, prefixes, directory layouts etc., since if one knows the version number, it means the metadata files have already been located. How to maintain the version numbers \u00b6 The version numbers are natural numbers. Each release can change the version number by at most one. In particular, if 2 (or more) changes were made to the specification but no release happened between them, they will be considered as belonging to the same version. Note: Terminology - The terms \"KV Store\", \"KV Metadata\", \"Spark Schema Metadata\", despite having different meaning usually, will be used interchangeably to mean the structure Spark Schema Metadata, which in itself is assigned per-column. we will use them in the context of describing what metadata for which column is laid out in what way. We will describe the structure of the spark schema, Spark's per-column metadata, and use Spark types. this will allow us to be detached from how spark actually represents these structures in the Parquet schema. Format Specifications \u00b6 Version 3 \u00b6 This version differs from Version 2 by: Configuration parameters prefix changed from com.ibm.metaindex to io.xskipper The index parameters are now stored as a map Version 2 \u00b6 This version differs from Version 1 by: Column name generation: For each index \\(I\\) (assume \\(I\\) is the name), defined on columns \\(c_1,...,c_n\\) (in this order), for each \\(c_i\\) , let \\(c'_i\\) denote \\(c_i\\) with the following changes, in this order: Replace all # with ## Replace all . with $#$ The column name will be: \\(c'_1c'_2...c'_n\\_I\\) len(c'_1)-len(c'_2)-...-len(c'_n) That is, the transformed column names concatenated, followed by the index name, followed by the lengths of the transformed column names concatenated with - as the delimiter Example of such transformation: SomeIndex on \"lat#_.$_new\" and \"$_lng.#\" will get lat##_$#$$_new_$_lng$#$##_someindex_14-10 since \"lat#_.$_new\" will be mapped to \"lat##_$#$$_new\" with length 14, and \"$_lng.#\" will be mapped to \"$_lng$#$##\" with length 10 Version 1 \u00b6 This version differs from version 0 by: the addition of PME Support. changes to the way column names are constructed from indexes moving tableIdentifier metadata field to be under the obj_name column Min/Max index is saved as nested field with native parquet types Value List is saved as parquet array type Notes: all additions were made in order for our library to be able to regenerate the encryption config (e.g., for refresh or during compaction). these configs are not used by PME itself (PME uses other fields in the parquet file itself, these are not available to us via spark). Theoretically it is possible to create a parquet file in which our metadata indicates an encryption config completely different than the one with which it's actually encrypted. we should avoid that. IMPORTANT no actual key material is ever written to the KV store, it's ALWAYS labels (e.g. encryption.column.keys ) As of version 0 (and 1), the set of column names in spark schema for all indexes is prefix free. this must remain the case, as the spark column names are used by our lib to derive the set of columns in the parquet schema for a specific index (e.g., a UDT translated to a column with a different name in the parquet schema). Changes: Additions to obj_name metadata: If the metadata is not encrypted, then no additions are made. If at least 1 index is encrypted, encryption metadata will be added the following way: key encryption of type spark.sql.types.Metadata , pointing to a metadata with with the following structure: key encryption.column.keys pointing to a String, containing the key list string for PME. the format of this string matches the format for the config with the same name used in PME. see this optional key \"encryption.plaintext.footer\" of type String, containing one of {true, false} , indicating whether or not plaintext footer is used if this key is not defined, then plaintext footer is implicitly disabled. key encryption.footer.key of type String, containing the footer key label (footer master Key ID in PME Terminology). this key is also used to encrypt the footer (the footer is always encrypted if encryption is on) this field is mandatory as a footer key is necessary if we use PME, even if plaintext footer mode is in use (the footer key is used only for signing in this case, and of course for obj_name ). Additions to each index metadata: Indexes which are not encrypted remain unchanged. For encrypted indexes, the following is added: key key_metadata of type String, pointing to the label of the key used to encrypt the set of columns for this index. The set of columns for a specific is obtained by acquiring the Parquet schema tree, and taking all the paths to leaves which start with the Spark column name for this index (this is why the set of spark column names must be prefix free). for example, for a MinMax on temp , the set of columns we need to encrypt is {temp_minmax.metadata} Note that this key label must be consistent with the one with which the columns for this index are encrypted, as configured in the column key list string in the obj_name metadata. if they are inconsistent, then this is a bug in the lib. the column key list string is kept in the obj_name metadata to save unnecessary scans to re-create that config when refreshing/compacting. the key_metadata in each index is used e.g. when listing existing indexes (to be able to retrieve the keyMetadata field in the Index case class). Column names for indexes are generated the same as in version 0, but the delimiter is now _ and not : , so for example, a SomeIndex over a,b would have gotten the column name a:b_someindex in version 0, now gets the column name a_b_someindex tableIdentifier metadata field is now saved only under the obj_name column (removed from index columns). Min/Max index is saved by having a nested field with min and max subfields each containing the value in native parquet type. Value List index is saved by saving an array data type Version 0 \u00b6 The metadata is represented by one row for each object, with the object name in its own column, and the metadata for each index in its own column as well, with the actual content of the metadata being the serialization of the UDT for this specific Metadata type. obj_name column: stores the object name. For Unversioned files, defined as StructField ( \"obj_name\" , StringType , false ) > That is, a non-nullable column named \"obj_name\" of type String, without metadata. For Versioned files (that is, version 0), defined as: val objNameMeta = new sql . types . MetadataBuilder () . putLong ( \"version\" , 0 ) . build () StructField ( \"obj_name\" , StringType , false , objNameMeta ) > That is, a non-nullable column named \"obj_name\" of type String, with Metadata containing a single key, \"version\", that points to a Long Per-Index Columns: For each index \\(I\\) (assume \\(I\\) is the name), defined on columns \\(c_1,...,c_n\\) (in this order), with UDT type T and params given as params : Map [ String , String ] A column with the following properties is defined: name : \\(c_1,...,c_n\\) _ \\(I\\) that is, the column names concatenated with a \":\" delimiter, followed by the index name concatenated with _ dataType : T That is, the UDT associated with this index. nullable - true metadata , a single key named index , pointing to another spark.sql.types.Metadata with the following structure: * key `cols`, pointing to a `java.lang.String[]` containing the index columns. * key `name`, pointing to the String `I` * key `tableIdentifier`, pointing to String generated from the URI in the following manner: - if the URI's Scheme is `COS`, then `<bucket_name>/<object_name>` - else, if the path for this URI (obtained by `new URI(uri).getPath()`) starts with a \"/\", the the preceding / is trimmed from this path, else it's unchanged. * Optional if `params` is not empty, then a key `params` points to `params`. Unversioned Files \u00b6 The Layout of the KV Store had several incarnations before it was versioned, so if looking at a metadata file (or group of files) without a version number, we will implicitly treat them as version 0, which will act as the \"as-built drawing\" for the KV Store layout, as of the time the version number was introduced. It's not defined what will happen should we encounter a file without a version, with KV Layout other than version 0.","title":"Parquet metadatastore spec"},{"location":"api/developer/parquet-metadatastore-spec/#parquet-metadatastore-spec","text":"This is a specification for how metadata is represented in the Parquet Metadata store. We support multiple versions and document them here. Note that the version number is stored in metadata files in a specific column of their Spark schema. Behind the scenes it is saved in the KV Metadata of the resulting Parquet files. This means that the version number can never imply metadata file locations, prefixes, directory layouts etc., since if one knows the version number, it means the metadata files have already been located.","title":"Parquet Metadatastore Spec"},{"location":"api/developer/parquet-metadatastore-spec/#how-to-maintain-the-version-numbers","text":"The version numbers are natural numbers. Each release can change the version number by at most one. In particular, if 2 (or more) changes were made to the specification but no release happened between them, they will be considered as belonging to the same version. Note: Terminology - The terms \"KV Store\", \"KV Metadata\", \"Spark Schema Metadata\", despite having different meaning usually, will be used interchangeably to mean the structure Spark Schema Metadata, which in itself is assigned per-column. we will use them in the context of describing what metadata for which column is laid out in what way. We will describe the structure of the spark schema, Spark's per-column metadata, and use Spark types. this will allow us to be detached from how spark actually represents these structures in the Parquet schema.","title":"How to maintain the version numbers"},{"location":"api/developer/parquet-metadatastore-spec/#format-specifications","text":"","title":"Format Specifications"},{"location":"api/developer/parquet-metadatastore-spec/#version-3","text":"This version differs from Version 2 by: Configuration parameters prefix changed from com.ibm.metaindex to io.xskipper The index parameters are now stored as a map","title":"Version 3"},{"location":"api/developer/parquet-metadatastore-spec/#version-2","text":"This version differs from Version 1 by: Column name generation: For each index \\(I\\) (assume \\(I\\) is the name), defined on columns \\(c_1,...,c_n\\) (in this order), for each \\(c_i\\) , let \\(c'_i\\) denote \\(c_i\\) with the following changes, in this order: Replace all # with ## Replace all . with $#$ The column name will be: \\(c'_1c'_2...c'_n\\_I\\) len(c'_1)-len(c'_2)-...-len(c'_n) That is, the transformed column names concatenated, followed by the index name, followed by the lengths of the transformed column names concatenated with - as the delimiter Example of such transformation: SomeIndex on \"lat#_.$_new\" and \"$_lng.#\" will get lat##_$#$$_new_$_lng$#$##_someindex_14-10 since \"lat#_.$_new\" will be mapped to \"lat##_$#$$_new\" with length 14, and \"$_lng.#\" will be mapped to \"$_lng$#$##\" with length 10","title":"Version 2"},{"location":"api/developer/parquet-metadatastore-spec/#version-1","text":"This version differs from version 0 by: the addition of PME Support. changes to the way column names are constructed from indexes moving tableIdentifier metadata field to be under the obj_name column Min/Max index is saved as nested field with native parquet types Value List is saved as parquet array type Notes: all additions were made in order for our library to be able to regenerate the encryption config (e.g., for refresh or during compaction). these configs are not used by PME itself (PME uses other fields in the parquet file itself, these are not available to us via spark). Theoretically it is possible to create a parquet file in which our metadata indicates an encryption config completely different than the one with which it's actually encrypted. we should avoid that. IMPORTANT no actual key material is ever written to the KV store, it's ALWAYS labels (e.g. encryption.column.keys ) As of version 0 (and 1), the set of column names in spark schema for all indexes is prefix free. this must remain the case, as the spark column names are used by our lib to derive the set of columns in the parquet schema for a specific index (e.g., a UDT translated to a column with a different name in the parquet schema). Changes: Additions to obj_name metadata: If the metadata is not encrypted, then no additions are made. If at least 1 index is encrypted, encryption metadata will be added the following way: key encryption of type spark.sql.types.Metadata , pointing to a metadata with with the following structure: key encryption.column.keys pointing to a String, containing the key list string for PME. the format of this string matches the format for the config with the same name used in PME. see this optional key \"encryption.plaintext.footer\" of type String, containing one of {true, false} , indicating whether or not plaintext footer is used if this key is not defined, then plaintext footer is implicitly disabled. key encryption.footer.key of type String, containing the footer key label (footer master Key ID in PME Terminology). this key is also used to encrypt the footer (the footer is always encrypted if encryption is on) this field is mandatory as a footer key is necessary if we use PME, even if plaintext footer mode is in use (the footer key is used only for signing in this case, and of course for obj_name ). Additions to each index metadata: Indexes which are not encrypted remain unchanged. For encrypted indexes, the following is added: key key_metadata of type String, pointing to the label of the key used to encrypt the set of columns for this index. The set of columns for a specific is obtained by acquiring the Parquet schema tree, and taking all the paths to leaves which start with the Spark column name for this index (this is why the set of spark column names must be prefix free). for example, for a MinMax on temp , the set of columns we need to encrypt is {temp_minmax.metadata} Note that this key label must be consistent with the one with which the columns for this index are encrypted, as configured in the column key list string in the obj_name metadata. if they are inconsistent, then this is a bug in the lib. the column key list string is kept in the obj_name metadata to save unnecessary scans to re-create that config when refreshing/compacting. the key_metadata in each index is used e.g. when listing existing indexes (to be able to retrieve the keyMetadata field in the Index case class). Column names for indexes are generated the same as in version 0, but the delimiter is now _ and not : , so for example, a SomeIndex over a,b would have gotten the column name a:b_someindex in version 0, now gets the column name a_b_someindex tableIdentifier metadata field is now saved only under the obj_name column (removed from index columns). Min/Max index is saved by having a nested field with min and max subfields each containing the value in native parquet type. Value List index is saved by saving an array data type","title":"Version 1"},{"location":"api/developer/parquet-metadatastore-spec/#version-0","text":"The metadata is represented by one row for each object, with the object name in its own column, and the metadata for each index in its own column as well, with the actual content of the metadata being the serialization of the UDT for this specific Metadata type. obj_name column: stores the object name. For Unversioned files, defined as StructField ( \"obj_name\" , StringType , false ) > That is, a non-nullable column named \"obj_name\" of type String, without metadata. For Versioned files (that is, version 0), defined as: val objNameMeta = new sql . types . MetadataBuilder () . putLong ( \"version\" , 0 ) . build () StructField ( \"obj_name\" , StringType , false , objNameMeta ) > That is, a non-nullable column named \"obj_name\" of type String, with Metadata containing a single key, \"version\", that points to a Long Per-Index Columns: For each index \\(I\\) (assume \\(I\\) is the name), defined on columns \\(c_1,...,c_n\\) (in this order), with UDT type T and params given as params : Map [ String , String ] A column with the following properties is defined: name : \\(c_1,...,c_n\\) _ \\(I\\) that is, the column names concatenated with a \":\" delimiter, followed by the index name concatenated with _ dataType : T That is, the UDT associated with this index. nullable - true metadata , a single key named index , pointing to another spark.sql.types.Metadata with the following structure: * key `cols`, pointing to a `java.lang.String[]` containing the index columns. * key `name`, pointing to the String `I` * key `tableIdentifier`, pointing to String generated from the URI in the following manner: - if the URI's Scheme is `COS`, then `<bucket_name>/<object_name>` - else, if the path for this URI (obtained by `new URI(uri).getPath()`) starts with a \"/\", the the preceding / is trimmed from this path, else it's unchanged. * Optional if `params` is not empty, then a key `params` points to `params`.","title":"Version 0"},{"location":"api/developer/parquet-metadatastore-spec/#unversioned-files","text":"The Layout of the KV Store had several incarnations before it was versioned, so if looking at a metadata file (or group of files) without a version number, we will implicitly treat them as version 0, which will act as the \"as-built drawing\" for the KV Store layout, as of the time the version number was introduced. It's not defined what will happen should we encounter a file without a version, with KV Layout other than version 0.","title":"Unversioned Files"},{"location":"concepts/data-skipping/","text":"Data Skipping \u00b6 Note This section explains the concepts that are use in Xskipper. For usage details go to the Getting Started Page . Data skipping can significantly boost the performance of SQL queries by skipping over irrelevant data objects or files based on summary metadata associated with each object. For every column in the object, the summary metadata might include minimum and maximum values, a list or bloom filter of the appearing values, or other metadata which succinctly represents the data in that column. We call this metadata a data skipping index (or simply index), and it is used during query evaluation to skip over objects which have no relevant data. Xskipper supports all of Spark's native data formats, including Parquet, ORC, CSV, JSON and Avro. Data skipping is a performance optimization feature which means that using data skipping does not affect the content of query results. Xskipper can be used to easily define new data skipping index types using a concept we call Extensible Data Skipping . For more information about usage see: Quick Start Guide Example Notebooks API Reference","title":"Data Skipping"},{"location":"concepts/data-skipping/#data-skipping","text":"Note This section explains the concepts that are use in Xskipper. For usage details go to the Getting Started Page . Data skipping can significantly boost the performance of SQL queries by skipping over irrelevant data objects or files based on summary metadata associated with each object. For every column in the object, the summary metadata might include minimum and maximum values, a list or bloom filter of the appearing values, or other metadata which succinctly represents the data in that column. We call this metadata a data skipping index (or simply index), and it is used during query evaluation to skip over objects which have no relevant data. Xskipper supports all of Spark's native data formats, including Parquet, ORC, CSV, JSON and Avro. Data skipping is a performance optimization feature which means that using data skipping does not affect the content of query results. Xskipper can be used to easily define new data skipping index types using a concept we call Extensible Data Skipping . For more information about usage see: Quick Start Guide Example Notebooks API Reference","title":"Data Skipping"},{"location":"concepts/extensible/","text":"Extensible Data Skipping \u00b6 Read the IEEE Big Data 2020 paper - Extensible Data Skipping Xskipper creates a level of abstraction between index creation/query evaluation, and the actual metadata store implementation, which is unique to each metadata store. This abstraction operates in two main areas: Indexing Flow - abstract metadata is generated during index creation and refresh by analyzing a DataFrame. Query Evaluation Flow - abstract metadata clauses are generated by filters that analyze the Catalyst (Spark optimizer) expression tree for pushdown predicates, and identify subtrees that can be mapped to a metadata Clause. These abstract structures (MetaData and Clauses) are then translated to a representation that matches a specific metadata store. Metadata Clauses can then be applied to MetaDataTypes in an efficient manner.","title":"Extensible Data Skipping"},{"location":"concepts/extensible/#extensible-data-skipping","text":"Read the IEEE Big Data 2020 paper - Extensible Data Skipping Xskipper creates a level of abstraction between index creation/query evaluation, and the actual metadata store implementation, which is unique to each metadata store. This abstraction operates in two main areas: Indexing Flow - abstract metadata is generated during index creation and refresh by analyzing a DataFrame. Query Evaluation Flow - abstract metadata clauses are generated by filters that analyze the Catalyst (Spark optimizer) expression tree for pushdown predicates, and identify subtrees that can be mapped to a metadata Clause. These abstract structures (MetaData and Clauses) are then translated to a representation that matches a specific metadata store. Metadata Clauses can then be applied to MetaDataTypes in an efficient manner.","title":"Extensible Data Skipping"},{"location":"concepts/indexing-flow/","text":"Indexing Flow \u00b6 Definitions \u00b6 Index \u00b6 An Index is a collection of data skipping metadata. In general data skipping metadata can enable skipping row or column subsets of a dataset. In our case we skip data at object/file granularity. Metadata Generator \u00b6 A component which generates abstract metadata by processing an object/file. The metadata is abstract in the sense that it's defined in memory. The concrete storage format for the metadata is implemented by the Metadata Translator. Metadata Translator \u00b6 A component which translates the metadata created by the index to a suitable format in order to be stored in the metadatastore. Index Creation Flow \u00b6 Index creation runs in 2 phases: Generaring abstract metadata types which hold the metadata in memory. Translating the abstract metadata types to a metadatastore representation and storing it. Xskipper currently supports a parquet metadata store which stores the metadata in parquet files. For more information about the parquet metadata store see here .","title":"Indexing Flow"},{"location":"concepts/indexing-flow/#indexing-flow","text":"","title":"Indexing Flow"},{"location":"concepts/indexing-flow/#definitions","text":"","title":"Definitions"},{"location":"concepts/indexing-flow/#index","text":"An Index is a collection of data skipping metadata. In general data skipping metadata can enable skipping row or column subsets of a dataset. In our case we skip data at object/file granularity.","title":"Index"},{"location":"concepts/indexing-flow/#metadata-generator","text":"A component which generates abstract metadata by processing an object/file. The metadata is abstract in the sense that it's defined in memory. The concrete storage format for the metadata is implemented by the Metadata Translator.","title":"Metadata Generator"},{"location":"concepts/indexing-flow/#metadata-translator","text":"A component which translates the metadata created by the index to a suitable format in order to be stored in the metadatastore.","title":"Metadata Translator"},{"location":"concepts/indexing-flow/#index-creation-flow","text":"Index creation runs in 2 phases: Generaring abstract metadata types which hold the metadata in memory. Translating the abstract metadata types to a metadatastore representation and storing it. Xskipper currently supports a parquet metadata store which stores the metadata in parquet files. For more information about the parquet metadata store see here .","title":"Index Creation Flow"},{"location":"concepts/query-evaluation-flow/","text":"Query Evaluation Flow \u00b6 Definitions \u00b6 Clause \u00b6 We analyze Expression Trees and label tree nodes with Clauses . A Clause is a boolean condition that can be applied to a data subset (i.e, object) \\(S\\) , typically by inspecting its metadata. For a Clause \\(c\\) and a (boolean) query expression \\(e\\) , we say that \\(c\\) represents \\(e\\) (denoted by \\(c \\wr e\\) ), if for every object \\(S\\) , whenever there exists a row \\(r \\in S\\) that satisfies \\(e\\) , then \\(S\\) satisfies \\(c\\) . This means that if \\(S\\) does not satisfy \\(c\\) , then \\(S\\) can be safely skipped when evaluating the query expression \\(e\\) . For example, given the expression \\(e = temp > 101\\) the clause \\(c = \\max_{r \\in S} temp(r) > 101\\) represents \\(e\\) (denoted by \\(c \\wr e\\) ). Therefore, objects where \\(c = \\max_{r \\in S} temp(r) <= 101\\) can be safely skipped Filter \u00b6 The labeling process of Expression Trees is done using filters . An algorithm A is a filter if it performs the following action: When given an expression tree \\(e\\) as input, for every (boolean valued) vertex \\(v\\) in \\(e\\) , it adds a set of clauses \\(C\\) s.t \\(\\forall c \\in C\\) : \\(c \\wr v\\) . For example, given the expression \\(e = temp > 101\\) : A filter \\(f\\) might label the Expression Tree using a MaxClause : MaxClause(c,>,v) is defined as \\(c = \\max_{r \\in S} c(r) > v\\) Where for a c is the column name v is the value. Since MaxClause(temperature,>,101) represents the node to which it was applied, \\(f\\) acted as a filter. Clause Translator \u00b6 A component which translates a Clause to a specific implementation according to the metadatastore type. Query Evaluation Flow \u00b6 Query evaluation is done in 2 phases: A query\u2019s Expression Tree \\(e\\) is labelled using a set of clauses The clauses are combined to provide a single clause which represents \\(e\\) . The labelling process is extensible, allowing for new index types and UDFs. The clause is translated to a form that can be applied at the metadata store to filter out objects which can be skipped during query run time. A simple example \u00b6 For example, given the query: SELECT * FROM employees WHERE salary > 5 AND name IN ( \u2018 Danny \u2019 , \u2019 Moshe \u2019 , \u2019 Yossi \u2019 ) The Expression Tree can be visualized as following: Assuming we have a MinMax Index for the salary column (store minimum and maximum values for each object) and a ValueList Index on the name column (storing the distinct list of values for each object). Applying the MinMax filter results in: Applying the ValueList filter on the results of the previous filter results in: Finally we generate a combined Abstract Clause: AND(MaxClause(salary, >, 5),ValueListClause(name, ('Danny', 'Moshe', 'Yossi'))) This clause will be translated to a form that can be applied at the metadata store to filter out objects which can be skipped during query run time","title":"Query Evaluation Flow"},{"location":"concepts/query-evaluation-flow/#query-evaluation-flow","text":"","title":"Query Evaluation Flow"},{"location":"concepts/query-evaluation-flow/#definitions","text":"","title":"Definitions"},{"location":"concepts/query-evaluation-flow/#clause","text":"We analyze Expression Trees and label tree nodes with Clauses . A Clause is a boolean condition that can be applied to a data subset (i.e, object) \\(S\\) , typically by inspecting its metadata. For a Clause \\(c\\) and a (boolean) query expression \\(e\\) , we say that \\(c\\) represents \\(e\\) (denoted by \\(c \\wr e\\) ), if for every object \\(S\\) , whenever there exists a row \\(r \\in S\\) that satisfies \\(e\\) , then \\(S\\) satisfies \\(c\\) . This means that if \\(S\\) does not satisfy \\(c\\) , then \\(S\\) can be safely skipped when evaluating the query expression \\(e\\) . For example, given the expression \\(e = temp > 101\\) the clause \\(c = \\max_{r \\in S} temp(r) > 101\\) represents \\(e\\) (denoted by \\(c \\wr e\\) ). Therefore, objects where \\(c = \\max_{r \\in S} temp(r) <= 101\\) can be safely skipped","title":"Clause"},{"location":"concepts/query-evaluation-flow/#filter","text":"The labeling process of Expression Trees is done using filters . An algorithm A is a filter if it performs the following action: When given an expression tree \\(e\\) as input, for every (boolean valued) vertex \\(v\\) in \\(e\\) , it adds a set of clauses \\(C\\) s.t \\(\\forall c \\in C\\) : \\(c \\wr v\\) . For example, given the expression \\(e = temp > 101\\) : A filter \\(f\\) might label the Expression Tree using a MaxClause : MaxClause(c,>,v) is defined as \\(c = \\max_{r \\in S} c(r) > v\\) Where for a c is the column name v is the value. Since MaxClause(temperature,>,101) represents the node to which it was applied, \\(f\\) acted as a filter.","title":"Filter"},{"location":"concepts/query-evaluation-flow/#clause-translator","text":"A component which translates a Clause to a specific implementation according to the metadatastore type.","title":"Clause Translator"},{"location":"concepts/query-evaluation-flow/#query-evaluation-flow_1","text":"Query evaluation is done in 2 phases: A query\u2019s Expression Tree \\(e\\) is labelled using a set of clauses The clauses are combined to provide a single clause which represents \\(e\\) . The labelling process is extensible, allowing for new index types and UDFs. The clause is translated to a form that can be applied at the metadata store to filter out objects which can be skipped during query run time.","title":"Query Evaluation Flow"},{"location":"concepts/query-evaluation-flow/#a-simple-example","text":"For example, given the query: SELECT * FROM employees WHERE salary > 5 AND name IN ( \u2018 Danny \u2019 , \u2019 Moshe \u2019 , \u2019 Yossi \u2019 ) The Expression Tree can be visualized as following: Assuming we have a MinMax Index for the salary column (store minimum and maximum values for each object) and a ValueList Index on the name column (storing the distinct list of values for each object). Applying the MinMax filter results in: Applying the ValueList filter on the results of the previous filter results in: Finally we generate a combined Abstract Clause: AND(MaxClause(salary, >, 5),ValueListClause(name, ('Danny', 'Moshe', 'Yossi'))) This clause will be translated to a form that can be applied at the metadata store to filter out objects which can be skipped during query run time","title":"A simple example"},{"location":"getting-started/quick-start-guide/","text":"Quick-Start Guide \u00b6 This guide helps you quickly get started using Xskipper with Apache Spark. Note For advanced details see the API section and the full API Reference . See here for sample notebooks. Setup Apache Spark with Xskipper \u00b6 Xskipper is compatible with Apache Spark 3.0. There are two ways to setup Xskipper: Run interactively: Start the Spark shell (Scala or Python) with Xskipper and explore Xskipper interactively. Run as a project: Set up a Maven or SBT project (Scala or Java) with Xskipper. Run with an interactive shell \u00b6 To use Xskipper interactively within the Spark\u2019s Scala/Python shell, you need a local installation of Apache Spark. Follow the instructions here to install Apache Spark. Spark Scala Shell \u00b6 Start a Spark Scala shell as follows: ./bin/spark-shell --packages io.xskipper:xskipper-core_2.12:1.2.0 PySpark \u00b6 Install or upgrade PySpark (3.0 or above) by running the following: pip install --upgrade pyspark Then, run PySpark with the Xskipper package: pyspark --packages io.xskipper:xskipper-core_2.12:1.2.0 Run as a project \u00b6 To build a project using the Xskipper binaries from the Maven Central Repository, use the following Maven coordinates: Maven \u00b6 Include Xskipper in a Maven project by adding it as a dependency in the project's POM file. Xskipper should be compiled with Scala 2.12. <dependency> <groupId> io.xskipper </groupId> <artifactId> xskipper-core_2.12 </artifactId> <version> 1.2.0 </version> </dependency> SBT \u00b6 Include Xskipper in an SBT project by adding the following line to its build.sbt file: libraryDependencies += \"io.xskipper\" %% \"xskipper-core\" % \"1.2.0\" Python \u00b6 To set up a Python project, first start the Spark session using the Xskipper package and then import the Python APIs. spark = pyspark . sql . SparkSession . builder . appName ( \"Xskipper\" ) \\ . config ( \"spark.jars.packages\" , \"io.xskipper:xskipper-core_2.12:1.2.0\" ) \\ . getOrCreate () from xskipper import Xskipper from xskipper import Registration Configure Xskipper \u00b6 In this example, we configure a JVM wide parameter to a base path which stores all data skipping indexes. The indexes can be stored on the same storage system as the data, but not under the same path. During query time indexes will be consulted at this location. For more configuration options, see configuration options . Python from xskipper import Xskipper # The base location to store all indexes # TODO: change to your index base location md_base_location = \"/tmp/metadata\" # Configuring the JVM wide parameters conf = dict ([ ( \"io.xskipper.parquet.mdlocation\" , md_base_location ), ( \"io.xskipper.parquet.mdlocation.type\" , \"EXPLICIT_BASE_PATH_LOCATION\" )]) Xskipper . setConf ( spark , conf ) Scala import io.xskipper._ import io.xskipper.implicits._ // The base location to store all indexes // TODO: change to your index base location val md_base_location = s\"/tmp/metadata\" // Configuring the JVM wide parameters val conf = Map ( \"io.xskipper.parquet.mdlocation\" -> md_base_location , \"io.xskipper.parquet.mdlocation.type\" -> \"EXPLICIT_BASE_PATH_LOCATION\" ) Xskipper . setConf ( conf ) Indexing a Dataset \u00b6 Creating a Sample Dataset \u00b6 First, let's create a sample dataset. Python from pyspark.sql.types import * # TODO: change to your data location dataset_location = \"/tmp/data\" df_schema = StructType ([ StructField ( \"dt\" , StringType (), True ), StructField ( \"temp\" , DoubleType (), True ), \\ StructField ( \"city\" , StringType (), True ), StructField ( \"vid\" , StringType (), True )]) data = [( \"2017-07-07\" , 20.0 , \"Tel-Aviv\" , \"a\" ), ( \"2017-07-08\" , 30.0 , \"Jerusalem\" , \"b\" )] df = spark . createDataFrame ( data , schema = df_schema ) # use partitionBy to make sure we have two objects df . write . partitionBy ( \"dt\" ) . mode ( \"overwrite\" ) . parquet ( dataset_location ) # read the dataset back from storage reader = spark . read . format ( \"parquet\" ) df = reader . load ( dataset_location ) df . show ( 10 , False ) Scala import org.apache.spark.sql.Row import org.apache.spark.sql.types. { DoubleType , StringType , StructField , StructType } // TODO: change to your data location val dataset_location = s\"/tmp/data\" val schema = List ( StructField ( \"dt\" , StringType , true ), StructField ( \"temp\" , DoubleType , true ), StructField ( \"city\" , StringType , true ), StructField ( \"vid\" , StringType , true ) ) val data = Seq ( Row ( \"2017-07-07\" , 20.0 , \"Tel-Aviv\" , \"a\" ), Row ( \"2017-07-08\" , 30.0 , \"Jerusalem\" , \"b\" ) ) val ds = spark . createDataFrame ( spark . sparkContext . parallelize ( data ), StructType ( schema ) ) // use partitionBy to make sure we have two objects ds . write . partitionBy ( \"dt\" ). mode ( \"overwrite\" ). parquet ( dataset_location ) // read the dataset back from storage val reader = spark . read . format ( \"parquet\" ) val df = reader . load ( dataset_location ) df . show ( false ) Indexing \u00b6 When creating a data skipping index on a data set, first decide which columns to index, then choose an index type for each column. These choices are workload and data dependent. Typically, choose columns to which predicates are applied in many queries. The following index types are supported out of the box: Index type Description Applicable to predicates in WHERE clauses Column types MinMax Stores minimum and maximum values for a column <,<=,=,>=,> All types except for complex types. See Supported Spark SQL data types . ValueList Stores the list of unique values for the column =,IN,LIKE All types except for complex types. See Supported Spark SQL data types . BloomFilter Uses a bloom filter to test for set membership =,IN Byte, String, Long, Integer, Short MinMax results in small index size and is a good usually choice when the dataset's sort order is correlated with a given column. For the other 2 options, choose value list if the number of distinct values in an object is typically much smaller than the total number of values in that object. On the other hand Bloom filters are recommended for columns with high cardinality (otherwise the index can get as big as that column of the data set). Note that Xskipper also enables to create your own data skipping indexes and specify how to use them during query time. For more details see here . Python # create Xskipper instance for the sample dataset xskipper = Xskipper ( spark , dataset_location ) # remove index if exists if xskipper . isIndexed (): xskipper . dropIndex () xskipper . indexBuilder () \\ . addMinMaxIndex ( \"temp\" ) \\ . addValueListIndex ( \"city\" ) \\ . addBloomFilterIndex ( \"vid\" ) \\ . build ( reader ) \\ . show ( 10 , False ) Scala // create Xskipper instance for the sample dataset val xskipper = new Xskipper ( spark , dataset_location ) // remove existing index if needed if ( xskipper . isIndexed ()) { xskipper . dropIndex () } xskipper . indexBuilder () . addMinMaxIndex ( \"temp\" ) . addValueListIndex ( \"city\" ) . addBloomFilterIndex ( \"vid\" ) . build ( reader ) . show ( false ) Viewing index status \u00b6 The following code shows how a user can view the current index status to check which indexes exist on the dataset and whether the index is up-to-date. Python xskipper . describeIndex ( reader ) . show ( 10 , False ) Scala xskipper . describeIndex ( reader ). show ( false ) List Indexed datasets \u00b6 The following code shows how a user can view all indexed datasets under the current base location. Python Xskipper . listIndexes ( spark ) . show ( 10 , False ) Scala Xskipper . listIndexes ( spark ). show ( false ) Using Data Skipping Indexes \u00b6 Enable/Disable Xskipper \u00b6 Xskipper provides APIs to enable or disable index usage with Spark. By using the \"enable\" command, Xskipper optimization rules become visible to the Apache Spark optimizer and will be used in query optimization and execution. By using the \"disable' command, Xskipper optimization rules no longer apply during query optimization. Note that disabling Xskipper has no impact on created indexes, and they remain intact. Python # Enable Xskipper Xskipper . enable ( spark ) # Disable Xskipper Xskipper . disable ( spark ) # You can use the following to check whether the Xskipper is enabled if not Xskipper . isEnabled ( spark ): Xskipper . enable ( spark ) Scala // Enable Xskipper spark . enableXskipper () // Disable Xskipper spark . disableXskipper () // You can use the following to check whether the Xskipper is enabled if (! spark . isXskipperEnabled ()) { spark . enableXskipper () } Running Queries \u00b6 Once Xskipper has been enabled you can run queries (using either SQL or the DataFrame API) and enjoy the performance and cost benefits of data skipping. There will be no change to query results. First, let's create a temporary view: Python df = reader . load ( dataset_location ) df . createOrReplaceTempView ( \"sample\" ) Scala df . createOrReplaceTempView ( \"sample\" ) Example query using the MinMax index \u00b6 Python spark . sql ( \"select * from sample where temp < 30\" ) . show () Scala spark . sql ( \"select * from sample where temp < 30\" ). show () Inspecting query skipping stats \u00b6 Python Xskipper . getLatestQueryAggregatedStats ( spark ) . show ( 10 , False ) Scala Xskipper . getLatestQueryAggregatedStats ( spark ). show ( false ) Note: the above returns the accumulated data skipping statistics for all of the datasets which were involved in the query. If you want to inspect the stats for a specific dataset you can call the API below to get stats on the Xskipper instance: Python xskipper . getLatestQueryStats () . show ( 10 , False ) Scala xskipper . getLatestQueryStats (). show ( false ) For more examples see the sample notebooks Index Life Cycle \u00b6 The following operations can be used in order to maintain the index. Refresh Index \u00b6 Over time the index can become stale as new files are added/removed/modified from the dataset. In order to bring the index up-to-date you can call the refresh operation which will index the new/modified files and remove obsolete metadata. Note: The index will still be beneficial for files which didn't change since the last indexing time even without refreshing. First let's simulate addition of new data to the dataset: Python # adding new file to the dataset to simulate changes in the dataset update_data = [( \"2017-07-09\" , 25.0 , \"Beer-Sheva\" , \"c\" )] update_df = spark . createDataFrame ( update_data , schema = df_schema ) # append to the existing dataset update_df . write . partitionBy ( \"dt\" ) . mode ( \"append\" ) . parquet ( dataset_location ) Scala val update_data = Seq ( Row ( \"2017-07-09\" , 25.0 , \"Beer-Sheva\" , \"c\" ) ) val update_ds = spark . createDataFrame ( spark . sparkContext . parallelize ( update_data ), StructType ( schema ) ) // append to the existing dataset update_ds . write . partitionBy ( \"dt\" ). mode ( \"append\" ). parquet ( dataset_location ) Now, let's inspect the index status: Python xskipper . describeIndex ( reader ) . show ( 10 , False ) Scala xskipper . describeIndex ( reader ). show ( false ) In this case the index status will indicate that there are new files that are not indexed. Therefore we use the Refresh operation to update the metadata: Python xskipper . refreshIndex ( reader ) . show ( 10 , False ) Scala xskipper . refreshIndex ( reader ). show ( false ) Now you can run the describe operation again and see that the metadata is up to date. Drop Index \u00b6 In order to drop the index use the following API call: Python xskipper . dropIndex () Scala xskipper . dropIndex () Working with Hive tables \u00b6 Xskipper also supports skipping over hive tables. Note that indexing is for Hive tables with partitions. To index tables without partitions, index the physical location directly. The API for working with hive tables is similar to the API presented above with 2 main differences: The uri used in the Xskipper constructor is the table identifier with the form: <db>.<table> . The API calls do not require a DataFrameReader. For more information regarding the API see here . The index location for a hive table is resolved according to the following: If the table contains the parameter io.xskipper.parquet.mdlocation this value will be used as the index location. Otherwise, xskipper will look up the parameter io.xskipper.parquet.mdlocation in the table's database and will use it as the base index location for all tables. Note: During indexing, the index location parameter can be automatically added to the table properties if the xskipper instance is configured accordingly. For more info regarding the index location configuration see here . Setting the base index location in the database \u00b6 In this example we will set the base location in the database. Python alter_db_ddl = ( \"ALTER DATABASE default SET DBPROPERTIES ('io.xskipper.parquet.mdlocation'=' {0} ')\" ) . format ( md_base_location ) spark . sql ( alter_db_ddl ) Scala val alter_db_ddl = s\"ALTER DATABASE default SET DBPROPERTIES ('io.xskipper.parquet.mdlocation'=' ${ md_base_location } ')\" spark . sql ( alter_db_ddl ) Creating a Sample Hive Table \u00b6 Let's create a hive table on the dataset we created earlier: Python create_table_ddl = \"\"\"CREATE TABLE IF NOT EXISTS tbl ( \\ temp Double, city String, vid String, dt String ) USING PARQUET PARTITIONED BY (dt) LOCATION ' {0} '\"\"\" . format ( dataset_location ) spark . sql ( create_table_ddl ) # recover the partitions spark . sql ( \"ALTER TABLE tbl RECOVER PARTITIONS\" ) # verify the table was created spark . sql ( \"show tables\" ) . show ( 10 , False ) spark . sql ( \"show partitions tbl\" ) . show ( 10 , False ) Scala val create_table_ddl = s\"\"\"CREATE TABLE IF NOT EXISTS tbl ( |temp Double, |city String, |vid String, |dt String |) |USING PARQUET |PARTITIONED BY (dt) |LOCATION ' ${ dataset_location } ' |\"\"\" . stripMargin spark . sql ( create_table_ddl ) // Recover the table partitions spark . sql ( \"ALTER TABLE tbl RECOVER PARTITIONS\" ) // verify the table was created spark . sql ( \"show tables\" ). show ( false ) spark . sql ( \"show partitions tbl\" ). show ( false ) Indexing a Hive Table \u00b6 Note we use default.sample as the uri in the Xskipper constructor. Python # create an Xskipper instance for the sample Hive Table xskipper_hive = Xskipper ( spark , 'default.tbl' ) # remove index if exists if xskipper_hive . isIndexed (): xskipper_hive . dropIndex () xskipper_hive . indexBuilder () \\ . addMinMaxIndex ( \"temp\" ) \\ . addValueListIndex ( \"city\" ) \\ . addBloomFilterIndex ( \"vid\" ) \\ . build () \\ . show ( 10 , False ) Scala // create an Xskipper instance for the sample Hive Table val xskipper_hive = new Xskipper ( spark , \"default.tbl\" ) // remove existing index if needed if ( xskipper_hive . isIndexed ()) { xskipper_hive . dropIndex () } xskipper_hive . indexBuilder () . addMinMaxIndex ( \"temp\" ) . addValueListIndex ( \"city\" ) . addBloomFilterIndex ( \"vid\" ) . build () . show ( false ) Running Queries \u00b6 Once Xskipper has been enabled you can continue running queries (using either SQL or DataFrame API) and enjoy the benefits of data skipping. First, let's make sure Xskipper is enabled: Python # You can use the following to check whether the Xskipper is enabled if not Xskipper . isEnabled ( spark ): Xskipper . enable ( spark ) Scala // You can use the following to check whether the Xskipper is enabled if (! spark . isXskipperEnabled ()) { spark . enableXskipper () } Example query using the MinMax index \u00b6 Python spark . sql ( \"select * from tbl where temp < 30\" ) . show ( false ) Scala spark . sql ( \"select * from tbl where temp < 30\" ). show ( false ) Inspecting the query stats: Python Xskipper . getLatestQueryAggregatedStats ( spark ) . show ( 10 , False ) Scala Xskipper . getLatestQueryAggregatedStats ( spark ). show ( false ) Index Life Cycle - Hive Tables \u00b6 The API is similar to the dataset API but without the need for a reader instance. View the index status \u00b6 Python xskipper_hive . describeIndex () . show ( 10 , False ) Scala xskipper_hive . describeIndex (). show ( false ) Refresh Index \u00b6 Python xskipper_hive . refreshIndex () . show ( 10 , False ) Scala xskipper . refreshIndex ( reader ). show ( false ) Drop Index \u00b6 In order to drop the index use the following API call: Python xskipper_hive . dropIndex () Scala xskipper_hive . dropIndex ()","title":"Quick Start Guide"},{"location":"getting-started/quick-start-guide/#quick-start-guide","text":"This guide helps you quickly get started using Xskipper with Apache Spark. Note For advanced details see the API section and the full API Reference . See here for sample notebooks.","title":"Quick-Start Guide"},{"location":"getting-started/quick-start-guide/#setup-apache-spark-with-xskipper","text":"Xskipper is compatible with Apache Spark 3.0. There are two ways to setup Xskipper: Run interactively: Start the Spark shell (Scala or Python) with Xskipper and explore Xskipper interactively. Run as a project: Set up a Maven or SBT project (Scala or Java) with Xskipper.","title":"Setup Apache Spark with Xskipper"},{"location":"getting-started/quick-start-guide/#run-with-an-interactive-shell","text":"To use Xskipper interactively within the Spark\u2019s Scala/Python shell, you need a local installation of Apache Spark. Follow the instructions here to install Apache Spark.","title":"Run with an interactive shell"},{"location":"getting-started/quick-start-guide/#spark-scala-shell","text":"Start a Spark Scala shell as follows: ./bin/spark-shell --packages io.xskipper:xskipper-core_2.12:1.2.0","title":"Spark Scala Shell"},{"location":"getting-started/quick-start-guide/#pyspark","text":"Install or upgrade PySpark (3.0 or above) by running the following: pip install --upgrade pyspark Then, run PySpark with the Xskipper package: pyspark --packages io.xskipper:xskipper-core_2.12:1.2.0","title":"PySpark"},{"location":"getting-started/quick-start-guide/#run-as-a-project","text":"To build a project using the Xskipper binaries from the Maven Central Repository, use the following Maven coordinates:","title":"Run as a project"},{"location":"getting-started/quick-start-guide/#maven","text":"Include Xskipper in a Maven project by adding it as a dependency in the project's POM file. Xskipper should be compiled with Scala 2.12. <dependency> <groupId> io.xskipper </groupId> <artifactId> xskipper-core_2.12 </artifactId> <version> 1.2.0 </version> </dependency>","title":"Maven"},{"location":"getting-started/quick-start-guide/#sbt","text":"Include Xskipper in an SBT project by adding the following line to its build.sbt file: libraryDependencies += \"io.xskipper\" %% \"xskipper-core\" % \"1.2.0\"","title":"SBT"},{"location":"getting-started/quick-start-guide/#python","text":"To set up a Python project, first start the Spark session using the Xskipper package and then import the Python APIs. spark = pyspark . sql . SparkSession . builder . appName ( \"Xskipper\" ) \\ . config ( \"spark.jars.packages\" , \"io.xskipper:xskipper-core_2.12:1.2.0\" ) \\ . getOrCreate () from xskipper import Xskipper from xskipper import Registration","title":"Python"},{"location":"getting-started/quick-start-guide/#configure-xskipper","text":"In this example, we configure a JVM wide parameter to a base path which stores all data skipping indexes. The indexes can be stored on the same storage system as the data, but not under the same path. During query time indexes will be consulted at this location. For more configuration options, see configuration options . Python from xskipper import Xskipper # The base location to store all indexes # TODO: change to your index base location md_base_location = \"/tmp/metadata\" # Configuring the JVM wide parameters conf = dict ([ ( \"io.xskipper.parquet.mdlocation\" , md_base_location ), ( \"io.xskipper.parquet.mdlocation.type\" , \"EXPLICIT_BASE_PATH_LOCATION\" )]) Xskipper . setConf ( spark , conf ) Scala import io.xskipper._ import io.xskipper.implicits._ // The base location to store all indexes // TODO: change to your index base location val md_base_location = s\"/tmp/metadata\" // Configuring the JVM wide parameters val conf = Map ( \"io.xskipper.parquet.mdlocation\" -> md_base_location , \"io.xskipper.parquet.mdlocation.type\" -> \"EXPLICIT_BASE_PATH_LOCATION\" ) Xskipper . setConf ( conf )","title":"Configure Xskipper"},{"location":"getting-started/quick-start-guide/#indexing-a-dataset","text":"","title":"Indexing a Dataset"},{"location":"getting-started/quick-start-guide/#creating-a-sample-dataset","text":"First, let's create a sample dataset. Python from pyspark.sql.types import * # TODO: change to your data location dataset_location = \"/tmp/data\" df_schema = StructType ([ StructField ( \"dt\" , StringType (), True ), StructField ( \"temp\" , DoubleType (), True ), \\ StructField ( \"city\" , StringType (), True ), StructField ( \"vid\" , StringType (), True )]) data = [( \"2017-07-07\" , 20.0 , \"Tel-Aviv\" , \"a\" ), ( \"2017-07-08\" , 30.0 , \"Jerusalem\" , \"b\" )] df = spark . createDataFrame ( data , schema = df_schema ) # use partitionBy to make sure we have two objects df . write . partitionBy ( \"dt\" ) . mode ( \"overwrite\" ) . parquet ( dataset_location ) # read the dataset back from storage reader = spark . read . format ( \"parquet\" ) df = reader . load ( dataset_location ) df . show ( 10 , False ) Scala import org.apache.spark.sql.Row import org.apache.spark.sql.types. { DoubleType , StringType , StructField , StructType } // TODO: change to your data location val dataset_location = s\"/tmp/data\" val schema = List ( StructField ( \"dt\" , StringType , true ), StructField ( \"temp\" , DoubleType , true ), StructField ( \"city\" , StringType , true ), StructField ( \"vid\" , StringType , true ) ) val data = Seq ( Row ( \"2017-07-07\" , 20.0 , \"Tel-Aviv\" , \"a\" ), Row ( \"2017-07-08\" , 30.0 , \"Jerusalem\" , \"b\" ) ) val ds = spark . createDataFrame ( spark . sparkContext . parallelize ( data ), StructType ( schema ) ) // use partitionBy to make sure we have two objects ds . write . partitionBy ( \"dt\" ). mode ( \"overwrite\" ). parquet ( dataset_location ) // read the dataset back from storage val reader = spark . read . format ( \"parquet\" ) val df = reader . load ( dataset_location ) df . show ( false )","title":"Creating a Sample Dataset"},{"location":"getting-started/quick-start-guide/#indexing","text":"When creating a data skipping index on a data set, first decide which columns to index, then choose an index type for each column. These choices are workload and data dependent. Typically, choose columns to which predicates are applied in many queries. The following index types are supported out of the box: Index type Description Applicable to predicates in WHERE clauses Column types MinMax Stores minimum and maximum values for a column <,<=,=,>=,> All types except for complex types. See Supported Spark SQL data types . ValueList Stores the list of unique values for the column =,IN,LIKE All types except for complex types. See Supported Spark SQL data types . BloomFilter Uses a bloom filter to test for set membership =,IN Byte, String, Long, Integer, Short MinMax results in small index size and is a good usually choice when the dataset's sort order is correlated with a given column. For the other 2 options, choose value list if the number of distinct values in an object is typically much smaller than the total number of values in that object. On the other hand Bloom filters are recommended for columns with high cardinality (otherwise the index can get as big as that column of the data set). Note that Xskipper also enables to create your own data skipping indexes and specify how to use them during query time. For more details see here . Python # create Xskipper instance for the sample dataset xskipper = Xskipper ( spark , dataset_location ) # remove index if exists if xskipper . isIndexed (): xskipper . dropIndex () xskipper . indexBuilder () \\ . addMinMaxIndex ( \"temp\" ) \\ . addValueListIndex ( \"city\" ) \\ . addBloomFilterIndex ( \"vid\" ) \\ . build ( reader ) \\ . show ( 10 , False ) Scala // create Xskipper instance for the sample dataset val xskipper = new Xskipper ( spark , dataset_location ) // remove existing index if needed if ( xskipper . isIndexed ()) { xskipper . dropIndex () } xskipper . indexBuilder () . addMinMaxIndex ( \"temp\" ) . addValueListIndex ( \"city\" ) . addBloomFilterIndex ( \"vid\" ) . build ( reader ) . show ( false )","title":"Indexing"},{"location":"getting-started/quick-start-guide/#viewing-index-status","text":"The following code shows how a user can view the current index status to check which indexes exist on the dataset and whether the index is up-to-date. Python xskipper . describeIndex ( reader ) . show ( 10 , False ) Scala xskipper . describeIndex ( reader ). show ( false )","title":"Viewing index status"},{"location":"getting-started/quick-start-guide/#list-indexed-datasets","text":"The following code shows how a user can view all indexed datasets under the current base location. Python Xskipper . listIndexes ( spark ) . show ( 10 , False ) Scala Xskipper . listIndexes ( spark ). show ( false )","title":"List Indexed datasets"},{"location":"getting-started/quick-start-guide/#using-data-skipping-indexes","text":"","title":"Using Data Skipping Indexes"},{"location":"getting-started/quick-start-guide/#enabledisable-xskipper","text":"Xskipper provides APIs to enable or disable index usage with Spark. By using the \"enable\" command, Xskipper optimization rules become visible to the Apache Spark optimizer and will be used in query optimization and execution. By using the \"disable' command, Xskipper optimization rules no longer apply during query optimization. Note that disabling Xskipper has no impact on created indexes, and they remain intact. Python # Enable Xskipper Xskipper . enable ( spark ) # Disable Xskipper Xskipper . disable ( spark ) # You can use the following to check whether the Xskipper is enabled if not Xskipper . isEnabled ( spark ): Xskipper . enable ( spark ) Scala // Enable Xskipper spark . enableXskipper () // Disable Xskipper spark . disableXskipper () // You can use the following to check whether the Xskipper is enabled if (! spark . isXskipperEnabled ()) { spark . enableXskipper () }","title":"Enable/Disable Xskipper"},{"location":"getting-started/quick-start-guide/#running-queries","text":"Once Xskipper has been enabled you can run queries (using either SQL or the DataFrame API) and enjoy the performance and cost benefits of data skipping. There will be no change to query results. First, let's create a temporary view: Python df = reader . load ( dataset_location ) df . createOrReplaceTempView ( \"sample\" ) Scala df . createOrReplaceTempView ( \"sample\" )","title":"Running Queries"},{"location":"getting-started/quick-start-guide/#example-query-using-the-minmax-index","text":"Python spark . sql ( \"select * from sample where temp < 30\" ) . show () Scala spark . sql ( \"select * from sample where temp < 30\" ). show ()","title":"Example query using the MinMax index"},{"location":"getting-started/quick-start-guide/#inspecting-query-skipping-stats","text":"Python Xskipper . getLatestQueryAggregatedStats ( spark ) . show ( 10 , False ) Scala Xskipper . getLatestQueryAggregatedStats ( spark ). show ( false ) Note: the above returns the accumulated data skipping statistics for all of the datasets which were involved in the query. If you want to inspect the stats for a specific dataset you can call the API below to get stats on the Xskipper instance: Python xskipper . getLatestQueryStats () . show ( 10 , False ) Scala xskipper . getLatestQueryStats (). show ( false ) For more examples see the sample notebooks","title":"Inspecting query skipping stats"},{"location":"getting-started/quick-start-guide/#index-life-cycle","text":"The following operations can be used in order to maintain the index.","title":"Index Life Cycle"},{"location":"getting-started/quick-start-guide/#refresh-index","text":"Over time the index can become stale as new files are added/removed/modified from the dataset. In order to bring the index up-to-date you can call the refresh operation which will index the new/modified files and remove obsolete metadata. Note: The index will still be beneficial for files which didn't change since the last indexing time even without refreshing. First let's simulate addition of new data to the dataset: Python # adding new file to the dataset to simulate changes in the dataset update_data = [( \"2017-07-09\" , 25.0 , \"Beer-Sheva\" , \"c\" )] update_df = spark . createDataFrame ( update_data , schema = df_schema ) # append to the existing dataset update_df . write . partitionBy ( \"dt\" ) . mode ( \"append\" ) . parquet ( dataset_location ) Scala val update_data = Seq ( Row ( \"2017-07-09\" , 25.0 , \"Beer-Sheva\" , \"c\" ) ) val update_ds = spark . createDataFrame ( spark . sparkContext . parallelize ( update_data ), StructType ( schema ) ) // append to the existing dataset update_ds . write . partitionBy ( \"dt\" ). mode ( \"append\" ). parquet ( dataset_location ) Now, let's inspect the index status: Python xskipper . describeIndex ( reader ) . show ( 10 , False ) Scala xskipper . describeIndex ( reader ). show ( false ) In this case the index status will indicate that there are new files that are not indexed. Therefore we use the Refresh operation to update the metadata: Python xskipper . refreshIndex ( reader ) . show ( 10 , False ) Scala xskipper . refreshIndex ( reader ). show ( false ) Now you can run the describe operation again and see that the metadata is up to date.","title":"Refresh Index"},{"location":"getting-started/quick-start-guide/#drop-index","text":"In order to drop the index use the following API call: Python xskipper . dropIndex () Scala xskipper . dropIndex ()","title":"Drop Index"},{"location":"getting-started/quick-start-guide/#working-with-hive-tables","text":"Xskipper also supports skipping over hive tables. Note that indexing is for Hive tables with partitions. To index tables without partitions, index the physical location directly. The API for working with hive tables is similar to the API presented above with 2 main differences: The uri used in the Xskipper constructor is the table identifier with the form: <db>.<table> . The API calls do not require a DataFrameReader. For more information regarding the API see here . The index location for a hive table is resolved according to the following: If the table contains the parameter io.xskipper.parquet.mdlocation this value will be used as the index location. Otherwise, xskipper will look up the parameter io.xskipper.parquet.mdlocation in the table's database and will use it as the base index location for all tables. Note: During indexing, the index location parameter can be automatically added to the table properties if the xskipper instance is configured accordingly. For more info regarding the index location configuration see here .","title":"Working with Hive tables"},{"location":"getting-started/quick-start-guide/#setting-the-base-index-location-in-the-database","text":"In this example we will set the base location in the database. Python alter_db_ddl = ( \"ALTER DATABASE default SET DBPROPERTIES ('io.xskipper.parquet.mdlocation'=' {0} ')\" ) . format ( md_base_location ) spark . sql ( alter_db_ddl ) Scala val alter_db_ddl = s\"ALTER DATABASE default SET DBPROPERTIES ('io.xskipper.parquet.mdlocation'=' ${ md_base_location } ')\" spark . sql ( alter_db_ddl )","title":"Setting the base index location in the database"},{"location":"getting-started/quick-start-guide/#creating-a-sample-hive-table","text":"Let's create a hive table on the dataset we created earlier: Python create_table_ddl = \"\"\"CREATE TABLE IF NOT EXISTS tbl ( \\ temp Double, city String, vid String, dt String ) USING PARQUET PARTITIONED BY (dt) LOCATION ' {0} '\"\"\" . format ( dataset_location ) spark . sql ( create_table_ddl ) # recover the partitions spark . sql ( \"ALTER TABLE tbl RECOVER PARTITIONS\" ) # verify the table was created spark . sql ( \"show tables\" ) . show ( 10 , False ) spark . sql ( \"show partitions tbl\" ) . show ( 10 , False ) Scala val create_table_ddl = s\"\"\"CREATE TABLE IF NOT EXISTS tbl ( |temp Double, |city String, |vid String, |dt String |) |USING PARQUET |PARTITIONED BY (dt) |LOCATION ' ${ dataset_location } ' |\"\"\" . stripMargin spark . sql ( create_table_ddl ) // Recover the table partitions spark . sql ( \"ALTER TABLE tbl RECOVER PARTITIONS\" ) // verify the table was created spark . sql ( \"show tables\" ). show ( false ) spark . sql ( \"show partitions tbl\" ). show ( false )","title":"Creating a Sample Hive Table"},{"location":"getting-started/quick-start-guide/#indexing-a-hive-table","text":"Note we use default.sample as the uri in the Xskipper constructor. Python # create an Xskipper instance for the sample Hive Table xskipper_hive = Xskipper ( spark , 'default.tbl' ) # remove index if exists if xskipper_hive . isIndexed (): xskipper_hive . dropIndex () xskipper_hive . indexBuilder () \\ . addMinMaxIndex ( \"temp\" ) \\ . addValueListIndex ( \"city\" ) \\ . addBloomFilterIndex ( \"vid\" ) \\ . build () \\ . show ( 10 , False ) Scala // create an Xskipper instance for the sample Hive Table val xskipper_hive = new Xskipper ( spark , \"default.tbl\" ) // remove existing index if needed if ( xskipper_hive . isIndexed ()) { xskipper_hive . dropIndex () } xskipper_hive . indexBuilder () . addMinMaxIndex ( \"temp\" ) . addValueListIndex ( \"city\" ) . addBloomFilterIndex ( \"vid\" ) . build () . show ( false )","title":"Indexing a Hive Table"},{"location":"getting-started/quick-start-guide/#running-queries_1","text":"Once Xskipper has been enabled you can continue running queries (using either SQL or DataFrame API) and enjoy the benefits of data skipping. First, let's make sure Xskipper is enabled: Python # You can use the following to check whether the Xskipper is enabled if not Xskipper . isEnabled ( spark ): Xskipper . enable ( spark ) Scala // You can use the following to check whether the Xskipper is enabled if (! spark . isXskipperEnabled ()) { spark . enableXskipper () }","title":"Running Queries"},{"location":"getting-started/quick-start-guide/#example-query-using-the-minmax-index_1","text":"Python spark . sql ( \"select * from tbl where temp < 30\" ) . show ( false ) Scala spark . sql ( \"select * from tbl where temp < 30\" ). show ( false ) Inspecting the query stats: Python Xskipper . getLatestQueryAggregatedStats ( spark ) . show ( 10 , False ) Scala Xskipper . getLatestQueryAggregatedStats ( spark ). show ( false )","title":"Example query using the MinMax index"},{"location":"getting-started/quick-start-guide/#index-life-cycle-hive-tables","text":"The API is similar to the dataset API but without the need for a reader instance.","title":"Index Life Cycle - Hive Tables"},{"location":"getting-started/quick-start-guide/#view-the-index-status","text":"Python xskipper_hive . describeIndex () . show ( 10 , False ) Scala xskipper_hive . describeIndex (). show ( false )","title":"View the index status"},{"location":"getting-started/quick-start-guide/#refresh-index_1","text":"Python xskipper_hive . refreshIndex () . show ( 10 , False ) Scala xskipper . refreshIndex ( reader ). show ( false )","title":"Refresh Index"},{"location":"getting-started/quick-start-guide/#drop-index_1","text":"In order to drop the index use the following API call: Python xskipper_hive . dropIndex () Scala xskipper_hive . dropIndex ()","title":"Drop Index"},{"location":"getting-started/sample-notebooks/","text":"Demo Notebooks \u00b6 You can use the following notebooks in order to get Started quickly with Xskipper: Python Notebook Scala Notebook","title":"Demo Notebooks"},{"location":"getting-started/sample-notebooks/#demo-notebooks","text":"You can use the following notebooks in order to get Started quickly with Xskipper: Python Notebook Scala Notebook","title":"Demo Notebooks"}]}