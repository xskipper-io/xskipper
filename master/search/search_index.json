{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"api/api-reference/","title":"API Reference","text":""},{"location":"api/api-reference/#api-reference","title":"API Reference","text":"<p>Use the following links for the full programmatic API:</p> <ul> <li>Scala API docs</li> <li>Python API docs</li> </ul>"},{"location":"api/creating-new-plugin/","title":"Using The Extensible API","text":""},{"location":"api/creating-new-plugin/#using-the-extensible-api","title":"Using The Extensible API","text":""},{"location":"api/creating-new-plugin/#intro","title":"Intro","text":"<p>Note</p> <p>This page explains the Extensible Data Skipping framework API.  See the Concepts page for an explanation about the underlying concepts.  See here for the list of currently available plugins.</p> <p>Xskipper supports adding your index types and specifying your own data skipping logic in order to enjoy data skipping over UDFs and a variety of data types.</p> <p>The pluggability is split between two main areas:</p> <ul> <li> <p>Indexing Flow</p> <ul> <li>Interfaces for defining a new index - implementations of Index along with IndexFactory.</li> <li>Interface for specifying how to store the metadata in the metadatastore - implementations of MetaDataTranslator.</li> </ul> </li> <li> <p>Query Evaluation Flow </p> <ul> <li>Interfaces for defining how query expressions are mapped to abstract conditions on metadata - implementations of MetaDataFilter along with MetaDataFilterFactory.</li> <li>Interfaces for translating an abstract clause to a specific implementation according to the metadatastore - Implementations of ClauseTranslator.</li> </ul> </li> </ul> <p>A new plugin can contain one or more of the above implementations.</p> <p>This architecture enables the registration of components using multiple packages.</p>"},{"location":"api/creating-new-plugin/#using-existing-plugins","title":"Using Existing Plugins","text":"<p>To use a plugin, load the relevant implementations using the Registration module (Scala, Python).  For example:</p> PythonScala <pre><code>from xskipper import Registration\n\nRegistration.addMetadataFilterFactory(spark, 'io.xskipper.plugins.regex.filter.RegexValueListMetaDataFilterFactory')\n# Add IndexFactory\nRegistration.addIndexFactory(spark, 'io.xskipper.plugins.regex.index.RegexIndexFactory')\n# Add MetaDataTranslator\nRegistration.addMetaDataTranslator(spark, 'io.xskipper.plugins.regex.parquet.RegexValueListMetaDataTranslator')\n# Add ClauseTranslator\nRegistration.addClauseTranslator(spark, 'io.xskipper.plugins.regex.parquet.RegexValueListClauseTranslator')\n</code></pre> <pre><code>import io.xskipper._\nimport io.xskipper.plugins.regex.filter.RegexValueListMetaDataFilterFactory\nimport io.xskipper.plugins.regex.index.RegexIndexFactory\nimport io.xskipper.plugins.regex.parquet.{RegexValueListClauseTranslator, RegexValueListMetaDataTranslator}\n\n// registering the filter factories for user metadataFilters\nRegistration.addIndexFactory(RegexIndexFactory)\nRegistration.addMetadataFilterFactory(RegexValueListMetaDataFilterFactory)\nRegistration.addClauseTranslator(RegexValueListClauseTranslator)\nRegistration.addMetaDataTranslator(RegexValueListMetaDataTranslator)\n</code></pre> <p>For the full list of plugins see here.</p> <p></p> <p>Note</p> <p>When registering multiple plugins the order of registration for <code>IndexFactory</code>, <code>MetadataTranslator</code>, and <code>ClauseTranslator</code> matters. If two plugins define relevant translations or index creation for the same parameters the first one registered will be used.  In general, you should avoid having multiple plugins that behave differently for the same indexes, metadata types or clauses.</p> <p>In the following sections we explain how to use the above interfaces in order to create a new plugin.  The explanations will use examples from the sample plugin - xskipper-regex-plugin.</p> <p>The regex plugin enables indexing a text column by specifying a list of patterns and saving the matching substrings as a value list.</p> <p>For example, consider an application log dataset and one of its objects : <pre><code>application_name,log_line\nbatch job,20/12/29 18:04:39 INFO FileSourceStrategy: Pruning directories with:\nbatch job,20/12/29 18:04:40 INFO DAGScheduler: ResultStage 22 (collect at ParquetMetadataHandle.scala:324) finished in 0.011 s\n</code></pre></p> <p>and the regex pattern <code>\".* .* .* (.*): .*\"</code>.</p> <p>When we index using the regex index, the metadata that will be saved is <code>List(\"FileSourceStrategy\", \"DAGScheduler\")</code>.</p> <p>The following query will benefit from this index and will skip the above object: <pre><code>SELECT * \nFROM tbl \nWHERE \nregexp_extract(log_line, '.* .* .* (.*): .*', 1) = 'MemoryStore'\n</code></pre></p>"},{"location":"api/creating-new-plugin/#indexing-flow","title":"Indexing Flow","text":""},{"location":"api/creating-new-plugin/#define-the-abstract-metadata","title":"Define the abstract metadata","text":"<p>Implementation(s) of MetaDataType</p> <p>First you need to define the abstract metadata type that will be generated by the index. This type will hold the metadata in memory.  For example, the MinMax index metadata type is a tuple of min and max values (see here)</p> <p>For the Regex plugin, to store the unique list of matching substrings for a given pattern we use a HashSet of Strings (see here).</p>"},{"location":"api/creating-new-plugin/#define-a-new-index","title":"Define a new index","text":"<p>Implementation(s) of Index along with IndexFactory</p> <p>Support for new indexes can be achieved by implementing a new class that implements the Index abstract class. A new index can use an existing MetaDataType or create its own MetaDataType along with a translation specification to the relevant metadatastore.</p> <p>The Index interface enables specifying one of two ways to collect the metadata:</p> <ul> <li> <p>Tree Reduce - in this code path the index processes the object row by row and updates its internal state to reflect the update to the metadata. This mode enables running index creation in parallel for multiple indexes.</p> </li> <li> <p>Optimized - using this interface the index processes the entire object DataFrame and generates the metadata.</p> </li> </ul> <p>For example, both the MinMaxIndex and the RegexValueListIndex. use the Tree Reduce mode to accumlate the list of unique matches.</p> <p>Along with the Index you need to define an IndexFactory -  the IndexFactory specifies how to recreate the index instance when loading the index parameters from the metadatastore.  For example, see RegexIndexFactory.</p>"},{"location":"api/creating-new-plugin/#define-translation-for-the-metadata","title":"Define translation for the metadata","text":"<p>Implementation(s) of MetaDataTranslator</p> <p>Info</p> <p>Xskipper uses by default Parquet as the metadatastore.   The Parquet metadatastore stores the metadata for the objects as rows in parquet files (for more details see here).     The API enables defining your own metadatastore. Here we focus on storing the metadata in the Parquet metadatastore. Therefore, the translations we cover here relate to the Parquet metadatastore.</p> <p>In order to store the abstract metadata defined above in the metadata store we have to specify a suitable translation which will map it to a valid representation for the metadatastore.</p> <p>For the Parquet metadatastore we have two options:</p> <ul> <li> <p>Convert the metadata to an internal Spark Row which will later be saved to Parquet automatically, as Spark supports Parquet out of the box. For example, the MinMax index translates its values to a nested row with <code>min</code> and <code>max</code> values (see here)</p> </li> <li> <p>Use a UDT to save the serialized abstract metadata. In some cases translating the metadata to Spark Row is not possible, therefore we save the metadata as a serialized binary.   To do so you have 2 options:</p> <ul> <li>Use the default java serialization provided by the parquet metadata store. To do so you need to define the UDT and register it. For example, for bloom filter, we use the following definition to get the default java serialization:</li> </ul> <pre><code>class BloomFilterMetaDataTypeUDT extends MetadataTypeUDT[BloomFilterMetaData]\n</code></pre> <p>Then register the UDT to Spark using the ParquetMetadataStoreUDTRegistration object: <pre><code>ParquetMetadataStoreUDTRegistration.registerUDT(classOf[BloomFilterMetaData].getName, classOf[BloomFilterMetaDataTypeUDT].getName)\n</code></pre></p> <p>Note that the bloom filter has the above definition built in so there is no need to register it.</p> <p>Note</p> <p>The UDT must be defined and registered in any program that uses the index.  A recommended pattern is to define and register the UDT in the Clause Translator object where you also define the Clause Translation logic.   This object will be loaded when registering the Clause Translator.</p> <ul> <li>Define your own UDT with custom serialization logic - similar to the above only this time you implement your own UDT.   See the <code>MetadataTypeUDT</code> class for a reference.</li> </ul> </li> </ul> <p>For the regex plugin we use the first option and translate the list of values to an array of values for storing in Parquet format (see here).</p>"},{"location":"api/creating-new-plugin/#query-evaluation-flow","title":"Query Evaluation Flow","text":""},{"location":"api/creating-new-plugin/#define-the-abstract-clause","title":"Define the abstract clause","text":"<p>Implementations of Clause</p> <p>First, you need to define the abstract clause that will be created by the Filter.   The Clause specifices an abstract condition which was deduced from the query and should operate on the metadata in order to determine the relevant objects. Each Clause is then translated to an explicit implementation according to the metadatastore type.   </p> <p>For example, for the MinMax index we define a MinMaxClause which follows the logic that was presented here.</p> <p>For the Regex Plugin we use a Clause which holds the required matching patterns from the query (see here).</p>"},{"location":"api/creating-new-plugin/#define-a-new-filter","title":"Define a new filter","text":"<p>Implementation(s) of MetaDataFilter along with MetaDataFilterFactory</p> <p>The filter processes the query tree and labels it with clauses. In most cases we would like to map expressions to clauses. Therefore, xskipper provides a basic implementation of a filter called BaseMetadataFilter which processes the query tree automatically for AND and OR operators, leaving the user to handle only the remaining expressions. Implementations which extend the BaseMetadataFilter need only specify how expressions are mapped to clauses.  For example,  RegexValueListFilter and MinMaxFilter map the query expressions according to the logic presented here.</p> <p>A more advanced filter can process the entire tree by implementing the <code>MetaDataFilter</code> class without using the <code>BaseMetadataFilter</code>.</p> <p>Along with a Filter you need to define a MetadataFilterFactory. The MetadataFilterFactory specifies which filters should run given the available indexes. For example, see the RegexIndexFactory.</p>"},{"location":"api/creating-new-plugin/#define-translation-for-the-clause","title":"Define translation for the clause","text":"<p>Implementations of ClauseTranslator</p> <p>Info</p> <p>Xskipper uses Parquet as the metadatastore by default.   The Parquet metadatastore stores the metadata for the objects as rows in parquet files (for more details see here).   Spark is used as the engine to run the abstract clauses on the metadata.    The API enables defining your own metadatastore. Here we focus on the metadata in the Parquet metadatastore. Therefore, the translations are relevant to the Parquet metadatastore.</p> <p>In order to process a clause, the abstract clause defined above needs to be translated to a form that is executable by the metadatastore.</p> <p>For the Parquet metadatastore we have 2 options:</p> <ul> <li> <p>Translate the Clause to a native Spark operation - this is useful when you have a built-in expression in Spark that can process the metadata. For example, for the MinMax index we use Spark\u2019s built-in inequality operators (&gt;, &lt;, &gt;=, &lt;=) to translate the abstract clause (see here).</p> </li> <li> <p>Use a UDF that will process the metadata - this is useful when the metadata is saved by serializing the abstract metadata type  or when there is no built-in operation that implements the logic needed in order to process the metadata.    For example, for the BloomFilter index which we serializes its metadata, we use a UDF to check whether the given value exists in the metadata or not (see here).</p> </li> </ul> <p>For the Regex Plugin we translate the clause to use Spark's <code>arrays_overlap</code> and <code>array_except</code> functions in order to check if the values in the clause exist in the metadata (see here).</p>"},{"location":"api/encryption/","title":"Index Encryption","text":""},{"location":"api/encryption/#encrypting-indexes","title":"Encrypting Indexes","text":"<p>When using Parquet Metadata Store, The metadata can optionally be encrypted using Parquet Modular Encryption (PME). This is achieved since the metadata itself is stored as a Parquet dataset, and thus PME can be used to encrypt it. This feature applies to all input formats, for example, a dataset stored in CSV format can have its metadata encrypted using PME.</p> <p>Note</p> <p>In the following sections, unless said otherwise, when referring to footers, columns etc., these are with respect to the metadata objects, and not the objects in the indexed dataset.</p>"},{"location":"api/encryption/#modularity","title":"Modularity","text":"<p>Index Encryption is modular and is granular in the following way:</p> <ul> <li>Each Index can either be encrypted (with a per-index key granularity) or left plaintext.</li> <li>Footer + object name column + partition key values:<ul> <li>The footer of the metadata object (in itself a Parquet file) contains (among other things):<ul> <li>Schema of the metadata object, which reveals the types, parameters and column names for all indexes collected (for example, one can learn that a Bloom Filter Index is defined on column <code>city</code> with false-positive probability <code>0.1</code>)</li> <li>Full path of the original dataset (or table name in case of a Hive Metastore table)</li> </ul> </li> <li>The object name column stores the names of all indexed objects, and their   modification timestamps at indexing time.</li> <li>The partition key values (for example, when Hive Style partitioning is used in the indexed dataset)   are automatically stored by Xskipper, each virtual column in its own dedicated column.</li> </ul> </li> </ul> <p>The footer + object name column + partition columns are encrypted using the same key - the \"footer key\", unless Plaintext footer is specified, in that case the object name column and the partition columns will still be encrypted, but the footer itself will be left plaintext (that is, the plaintext footer mode of PME will be activated). If at least 1 index is encrypted then the footer key must be set.</p> <p>This granularity enables usecases where different users have access to different subsets of the indexes in the metadata. In general, Xskipper will only try to access the indexes relevant for the query (based on the predicates in the <code>WHERE</code>  clause) - and thus it's enough to only have access to the footer key and the keys for the relevant indexes. Thus, with an appropriate key selection, a single copy of the metadata can be used by different users, with each user having the keys for the indexes relevant to the columns they have access to in the data.</p> <p>Warning</p> <p>Xskipper doesn't have a fallback mechanism for index access, so even if the user has access to a subset of the indexes that is able to provide some skipping, if there are usable indexes for the query which are unaccessible - Xskipper will not be able to provide any skipping, the query will succeed but no skipping will happen - for example, say we have a <code>MinMax</code> on <code>temp</code> and <code>ValueList</code> on <code>city</code> but we can only access the footer key + the key  for the <code>MinMax</code> and we run a query with <code>WHERE city = 'PARIS' AND temp &lt; 5</code> - even though we could use just the <code>MinMax</code> to get some skipping (though probably not all) - no skipping will occur.</p>"},{"location":"api/encryption/#how-to-choose-keys","title":"How to choose keys","text":"<p>As mentioned before, if an index is not required for a query, Xskipper won't try to access it, and so the only factor for whether Xskipper tries to access an index is whether it's required for a query - and NOT whether it's accessible. thus, a good practice would be to make sure that if someone has access to a column, they will also have access to the indexes defined for that column (and of course the footer).</p>"},{"location":"api/encryption/#usage-flow","title":"Usage Flow","text":"<p>Danger</p> <p>When using index encryption, whenever a \"key\" is configured in any Xskipper API, it's always the label - NEVER the key itself.</p> <p>In the following code examples we omit the PME configuration part as it varies between different implementations. A demonstrational KMS example can be found here</p>"},{"location":"api/encryption/#index-creation-flow","title":"Index Creation Flow","text":"<p>Once the metadata is created, subsequent usages (both querying and refreshing) are  all agnostic to the fact that this metadata is encrypted - so, except for the regular PME configurations (KMS, auth etc.) - everything remains the same.</p>"},{"location":"api/encryption/#sample-1-creating-metadata-with-encrypted-footer-default","title":"Sample 1 - Creating metadata with encrypted footer (default)","text":"PythonScala <pre><code># configure footer key\nconf = dict([(\"io.xskipper.parquet.encryption.footer.key\", \"k1\")])\nxskipper.setConf(conf)\n# adding the indexes\nxskipper.indexBuilder() \\\n.addMinMaxIndex(\"temp\", \"k2\") \\\n.addValueListIndex(\"city\") \\\n.build(reader) \\\n.show(10, False)\n</code></pre> <pre><code>// set the footer key\nval conf = Map(\n  \"io.xskipper.parquet.encryption.footer.key\" -&gt; \"k1\")\nxskipper.setConf(conf)\nxskipper\n  .indexBuilder()\n  // Add an encrypted MinMax index for temp\n  .addMinMaxIndex(\"temp\", \"k2\")\n  // Add a plaintext ValueList index for city\n  .addValueListIndex(\"city\")\n  .build(reader).show(false)\n</code></pre>"},{"location":"api/encryption/#sample-2-creating-metadata-with-plaintext-footer","title":"Sample 2 - creating metadata with plaintext footer","text":"PythonScala <pre><code># configure footer key\nconf = dict([(\"io.xskipper.parquet.encryption.footer.key\", \"k1\"),\n(\"io.xskipper.parquet.encryption.plaintext.footer\", \"true\")])\nxskipper.setConf(conf)\n# adding the indexes\nxskipper.indexBuilder() \\\n.addMinMaxIndex(\"temp\", \"k2\") \\\n.addValueListIndex(\"city\") \\\n.build(reader) \\\n.show(10, False)\n</code></pre> <pre><code>// set the footer key\nval conf = Map(\n\"io.xskipper.parquet.encryption.footer.key\" -&gt; \"k1\",\n\"io.xskipper.parquet.encryption.plaintext.footer\" -&gt; \"true\")\nxskipper.setConf(conf)\nxskipper\n.indexBuilder()\n// Add an encrypted MinMax index for temp\n.addMinMaxIndex(\"temp\", \"k2\")\n// Add a plaintext ValueList index for city\n.addValueListIndex(\"city\")\n.build(reader).show(false)\n</code></pre>"},{"location":"api/encryption/#query-flow","title":"Query Flow","text":"<p>When running Queries, there is no practical difference between encrypted metadata and non-encrypted metadata. The only addition is the need to configure PME.  </p>"},{"location":"api/encryption/#index-refresh-flow","title":"Index Refresh Flow","text":"<p>Note</p> <p>When refreshing encrypted metadata, ALL keys used in the metadata (that is, keys to all indexes + footer key) need to be accessible.</p> <p>When refreshing encrypted metadata, no action is required except for PME configurations (KMS, auth etc.)  </p>"},{"location":"api/indexing/","title":"Indexing","text":""},{"location":"api/indexing/#indexing","title":"Indexing","text":"<p>For every column in the object, Xskipper can collect summary metadata. This metadata is used during query evaluation to skip over objects which have no relevant data.</p>"},{"location":"api/indexing/#default-indexes","title":"Default Indexes","text":"<p>The following indexes are supported out of the box:</p> Index type Description Applicable to predicates in WHERE clauses Column types MinMax Stores minimum and maximum values for a column &lt;,&lt;=,=,&gt;=,&gt; All types except for complex types. See Supported Spark SQL data types. ValueList Stores the list of unique values for the column =,IN,LIKE All types except for complex types. See Supported Spark SQL data types. BloomFilter Uses a bloom filter to test for set membership =,IN Byte, String, Long, Integer, Short <p>In order to add an index using the <code>IndexBuilder</code> to specify the required indexes, for example:</p> PythonScala <pre><code># create Xskipper instance for the sample dataset\nxskipper = Xskipper(spark, dataset_location)\n\n# remove index if exists\nif xskipper.isIndexed():\n    xskipper.dropIndex()\n\nxskipper.indexBuilder() \\\n        .addMinMaxIndex(\"temp\") \\\n        .addValueListIndex(\"city\") \\\n        .addBloomFilterIndex(\"vid\") \\\n        .build(reader) \\\n        .show(10, False)\n</code></pre> <pre><code>// create Xskipper instance for the sample dataset\nval xskipper = new Xskipper(spark, dataset_location)\n\n// remove existing index if needed\nif (xskipper.isIndexed()) {\n  xskipper.dropIndex()\n}\n\nxskipper.indexBuilder()\n        .addMinMaxIndex(\"temp\")\n        .addValueListIndex(\"city\")\n        .addBloomFilterIndex(\"vid\")\n        .build(reader)\n        .show(false)\n</code></pre> <p>By default, the indexes are stored as parquet files stored in storage  Each parquet file with row per each object in the dataset.  </p> <p>For more information about the parquet metadatastore see here.</p>"},{"location":"api/indexing/#plugins","title":"Plugins","text":"<p>Xskipper supports adding new indexes using a pluggable system. For instructions on how to create a new plugin see here.</p>"},{"location":"api/indexing/#supported-plugins","title":"Supported plugins","text":"<p>Currently the following plugins are supported (in addition to the built-in indexes: MinMax, ValueList and BloomFilter):</p> <ul> <li>Regex Plugin - An index which enables to save a value list for a given regex.</li> </ul>"},{"location":"api/indexing/#setting-up-a-plugin","title":"Setting up a plugin","text":"<p>In order to use a plugin you first need to register the needed classes. For example, for the Regex Plugin:</p> PythonScala <pre><code>from xskipper import Registration\n\nRegistration.addMetadataFilterFactory(spark, 'io.xskipper.plugins.regex.filter.RegexValueListMetaDataFilterFactory')\n# Add IndexFactory\nRegistration.addIndexFactory(spark, 'io.xskipper.plugins.regex.index.RegexIndexFactory')\n# Add MetaDataTranslator\nRegistration.addMetaDataTranslator(spark, 'io.xskipper.plugins.regex.parquet.RegexValueListMetaDataTranslator')\n# Add ClauseTranslator\nRegistration.addClauseTranslator(spark, 'io.xskipper.plugins.regex.parquet.RegexValueListClauseTranslator')\n</code></pre> <pre><code>import io.xskipper._\nimport io.xskipper.plugins.regex.filter.RegexValueListMetaDataFilterFactory\nimport io.xskipper.plugins.regex.index.RegexIndexFactory\nimport io.xskipper.plugins.regex.parquet.{RegexValueListClauseTranslator, RegexValueListMetaDataTranslator}\n\n// registering the filter factories for user metadataFilters\nRegistration.addIndexFactory(RegexIndexFactory)\nRegistration.addMetadataFilterFactory(RegexValueListMetaDataFilterFactory)\nRegistration.addClauseTranslator(RegexValueListClauseTranslator)\nRegistration.addMetaDataTranslator(RegexValueListMetaDataTranslator)\n</code></pre>"},{"location":"api/indexing/#index-building","title":"Index building","text":"<p>In order to build an index you can use the <code>addCustomIndex</code> API.   </p> <p>For example for the Regex Plugin:</p> PythonScala <pre><code>    xskipper = Xskipper(spark, dataset_path)\n\n    # adding the index using the custom index API\n    xskipper.indexBuilder() \\\n        .addCustomIndex(\"io.xskipper.plugins.regex.index.RegexValueListIndex\", [\"log_line\"],\n                        {\"io.xskipper.plugins.regex.pattern.r0\": \".* .* .* (.*): .*\"}) \\\n        .build(reader) \\\n        .show(10, False)\n</code></pre> <pre><code>import io.xskipper.plugins.regex.implicits._\n\n// index the dataset\nval xskipper = new Xskipper(spark, dataset_path)\n\n\nxskipper\n  .indexBuilder()\n  // using the implicit method defined in the plugin implicits\n  .addRegexValueListIndex(\"log_line\", Seq(\".* .* .* (.*): .*\"))\n  // equivalent\n  //.addCustomIndex(RegexValueListIndex(\"log_line\", Seq(\".* .* .* (.*): .*\")))\n  .build(reader).show(false)\n</code></pre>"},{"location":"api/indexing/#creating-you-own-plugin","title":"Creating you own plugin","text":"<p>In order to create your own plugin see here.</p>"},{"location":"api/configuration/configuration/","title":"Xskipper Configuration","text":""},{"location":"api/configuration/configuration/#configuration","title":"Configuration","text":"<p>Xskipper enables to configure the instances using JVM wide configtation which applies for all created Xskipper instances. In addition one can set specific configuration for a given instance as described below.</p> <p>For the configuration related to the parquet metadatastore see here.</p>"},{"location":"api/configuration/configuration/#setting-jvm-wide-configuration","title":"Setting JVM wide configuration","text":"<p>Use the following to set JVM wide configuration properties for Xskipper instances. These configurations will be applied by default on each new instance.</p> PythonScala <pre><code>from xskipper import Xskipper\n# TODO: fill with config values\nconf = dict()\nXskipper.setConf(spark, conf)\n</code></pre> <pre><code>import io.xskipper._\n// TODO: fill with config values\nval conf = Map()\nXskipper.setConf(conf)\n</code></pre>"},{"location":"api/configuration/configuration/#setting-configuration-for-a-specific-xskipper-instance","title":"Setting configuration for a specific Xskipper instance","text":"<p>Use the following to set the configuration for a specific Xskipper instance</p> PythonScala <pre><code># call set params to make sure it overwrites JVM wide config\n# TODO: fill with config values\nparams = dict()\nxskipper.setParams(params)\n</code></pre> <pre><code>val xskipper = new Xskipper(spark, dataset_location)\n\n// call set params to make sure it overwrites JVM wide config\n// TODO: fill with config values\nval params = Map()\nxskipper.setParams(params)\n</code></pre>"},{"location":"api/configuration/configuration/#xskipper-properties","title":"Xskipper properties","text":"Property Default Description io.xskipper.evaluation.enabled false When true, queries will run in evaluation mode.When running in evaluation mode all of the indexed dataset will only be processed for skipping stats and no data will be read. The evaluation mode is useful when we want to inspect the skipping stats io.xskipper.timeout 10 The timeout in minutes to retrieve the indexed objects and the relevant objects for the query from the metadatastore io.xskipper.identifierclass io.xskipper.utils.identifier.Identifier The fully qualified name of an identifier class to be loaded using reflection used to specify how the table identifier and file ID are determined. For more information see here. io.xskipper.index.parallelism 10 defines the number of concurrent objects that will be indexed io.xskipper.index.minchunksize 1 Defines the minimum chunk size to be used when indexing.  The chunk size will be multiplied by 2 till reaching the metadataStore upload chunk size io.xskipper.index.bloom.fpp 0.01 Indicates the bloom filter default fpp io.xskipper.index.bloom.ndv 100000 Indicates the bloom filter expected number of distinct values io.xskipper.index.minmax.readoptimized.parquet true Indicates whether the collection of min/max stats for Numeric columns when working with Parquet objects will be done in an optimized way by reading the stats from the Parquet footer io.xskipper.index.minmax.readoptimized.parquet.parallelism 10000 The number of objects to be indexed in parallel when having only minmax indexes on parquet objects io.xskipper.index.minmax.inFilterThreshold 100 defines number of values in an IN filter above we will push down only one min/max condition based on the min and maximum of the entire list on parquet objects io.xskipper.index.memoryFraction 0.2 The memory fraction from the driver memory that indexing will use in order to determine the maximum chunk size dynamically"},{"location":"api/configuration/configuration/#identifier-class","title":"Identifier Class","text":"<p>Each dataset/table/file that that is indexed by Xskipper is identified by some Identifier. This identifier is used for in order to determine the following:</p> <ul> <li>The file ID for each indexed file - typically the file ID is comprised of the file name concatenated with the last modified time stamp to detect changes since last indexing time.</li> <li>Display name for identifiers</li> <li>Disply name for paths</li> </ul> <p>Currently Xskipper supports two implementation of the Identifer class:</p> <ul> <li> <p>Default Identifier</p> <ul> <li>The File ID is  - <code>&lt;file_name&gt;#&lt;last_modification_time&gt;</code></li> <li>The Identifier is the uri after removing a trailing <code>/</code> if exists.</li> <li>The Path and Display name are the given paths</li> </ul> </li> <li> <p>IBM COS Identifier - differs from the default identifier by</p> <ul> <li>The identifier for IBM COS paths when using Stocator is removing the <code>service</code> part and keeps the bucket name. So, for example the identifier for <code>cos://mybucket.service/path/to/dataset/</code> is <code>cos://mybucket/path/to/dataset</code> </li> </ul> </li> </ul> <p>For most cases you can use the default implementation, however if you require some custom logic you can add your own implementation for the Identifier class (for example, the IBM COS Identifier as an example). The Identifier class is by setting the paramater <code>io.xskipper.identifierclass</code> to the and determines the logic for inferring the identifier for each </p>"},{"location":"api/configuration/parquet-mdstore-configuration/","title":"Parquet Metadatastore Configuration","text":""},{"location":"api/configuration/parquet-mdstore-configuration/#parquet-metadatastore-properties","title":"Parquet Metadatastore properties","text":"<p>Xskipper uses Parquet as the metadatastore. The Parquet metadatastore store the metadata for the objects as rows in parquet file. See here for more details.</p> <p>The following are parameters relevant for the Parquet metadatastore.  </p> <p>These parameters can be set as:</p> <ul> <li>JVM wide configuration as described here.</li> <li>Specific Xskipper instance as described here.</li> </ul> Property Default Description io.xskipper.parquet.mdlocation N/A The metadata location according to the type io.xskipper.parquet.mdlocation.type EXPLICIT_BASE_PATH_LOCATION See here io.xskipper.parquet.index.chunksize 25000 The number of objects to index in each chunk io.xskipper.parquet.maxRecordsPerMetadataFile 50000 The number of records per metadata file used in the compact stage to limit the number of records for small sized metadata files. io.xskipper.parquet.maxMetadataFileSize 33554432 (32MB) The expected max size of each metadata file will be used by the compact function to distribute the data to multiple files to distribute the data to multiple files in such way that each is less than the expected max size of each metadata file used in conjunction with the max records per file configuration io.xskipper.parquet.encryption.plaintext.footer false Whether or not to use plain footer io.xskipper.parquet.encryption.footer.key N/A The encryption key that will be used to encrypt the metadata footer io.xskipper.parquet.refresh.dedup true When set to true each refresh operation will drop duplicates metadata entries (which might exist due to failed refresh operations) io.xskipper.parquet.filter.dedup false When set to true duplicate metadata entries (which might exist due to failed refresh operations) will be dropped when filtering in query run time <p>Note</p> <p>Deduplication during filtering is documented as <code>false</code> by default, but in version <code>1.2.3</code> it accidentally defaults to <code>true</code>. this cannot affect query results, but may degrade performance. as a workaround, it can be manually set <code>io.xskipper.parquet.filter.dedup</code> to <code>false</code>.</p>"},{"location":"api/configuration/parquet-mdstore-configuration/#types-of-metadata-location","title":"Types of metadata location","text":"<p>The parquet metadatastore looks up the metadata location according to the paramater <code>io.xskipper.parquet.mdlocation</code>, this parameter  is interpreted according to the URL type defined in the parameter <code>io.xskipper.parquet.mdlocation.type</code>.  </p> <p>The following options are available for the parameter <code>io.xskipper.parquet.mdlocation.type</code>:</p> <ul> <li><code>EXPLICIT_BASE_PATH_LOCATION</code>: This is the default. An explicit definition of the base path to the metadata, which is combined with a data set identifier. This case can be used to configure the Xskipper JVM wide settings and have all of data sets metadata saved under the base path.</li> <li><code>EXPLICIT_LOCATION</code>: An explicit full path to the metadata.</li> <li><code>HIVE_TABLE_NAME</code>: The name of the Hive table (in the form of <code>&lt;db&gt;.&lt;table&gt;</code>) that contains the exact path of the metadata in the table properties under the parameter <code>io.xskipper.parquet.mdlocation</code>.</li> <li><code>HIVE_DB_NAME</code>: The name of the Hive database that contains the base path of the metadata in the database properties under the parameter <code>io.xskipper.parquet.mdlocation</code>.</li> </ul> <p>For more information on how the metadata path is resolved see here.</p> <p>You should set the <code>io.xskipper.parquet.mdlocation</code> in one of two ways:</p>"},{"location":"api/configuration/parquet-mdstore-configuration/#setting-base-location-for-metadata-recommended","title":"Setting base location for metadata (recommended)","text":"<p>This configuration is useful for setting base location once for all datasets and should be used with the <code>EXPLICIT_BASE_PATH_LOCATION</code> or <code>HIVE_DB_NAME</code> types.</p> <p>The location of the metadata for each data set will be inferred automatically by combining the base path with a data set identifier.</p> <p>For example, setting <code>EXPLICIT_BASE_PATH_LOCATION</code>:</p> PythonScala <pre><code>from xskipper import Xskipper\n\n# The base location to store all indexes \n# TODO: change to your index base location\nmd_base_location = \"/tmp/metadata\"\n\n# Configuring the JVM wide parameters\nconf = dict([\n            (\"io.xskipper.parquet.mdlocation\", md_base_location),\n            (\"io.xskipper.parquet.mdlocation.type\", \"EXPLICIT_BASE_PATH_LOCATION\")])\nXskipper.setConf(spark, conf)\n</code></pre> <pre><code>import io.xskipper._\n// TODO: Change to your location\nval location = \"/tmp/metadata\"\nval conf = Map(\n  \"io.xskipper.parquet.mdlocation\" -&gt; location,\n  \"io.xskipper.parquet.mdlocation.type\" -&gt;  \"EXPLICIT_BASE_PATH_LOCATION\")\n// set JVM-wide parameters\nXskipper.setConf(conf)\n</code></pre>"},{"location":"api/configuration/parquet-mdstore-configuration/#setting-an-explicit-metadata-location-for-an-xskipper-instance","title":"Setting an explicit metadata location for an Xskipper instance","text":"<p>This configuration is useful for setting a specific metadata location for a certain data set and should be used with the <code>EXPLICIT_LOCATION</code> or <code>HIVE_TABLE_NAME</code> type.   You can also set it to <code>EXPLICIT_BASE_PATH_LOCATION</code> or <code>HIVE_DB_NAME</code> if you want to override the JVM defaults. </p> <p>For example setting an <code>EXPLICIT_LOCATION</code> for a specific xskipper instance:</p> PythonScala <pre><code>xskipper = Xskipper(spark_session, dataset_location)\n\n# call set params to make sure it overwrites JVM wide config\nparams = dict([\n    ('io.xskipper.parquet.mdlocation', '&lt;your metadata&gt;'),\n    ('io.xskipper.parquet.mdlocation.type', 'EXPLICIT_LOCATION')])\nxskipper.setParams(params)\n</code></pre> <pre><code>val xskipper = new Xskipper(spark, dataset_location)\n\n// call set params to make sure it overwrites JVM wide config\nval params = Map(\n  \"io.xskipper.parquet.mdlocation\" -&gt; \"&lt;your metadata&gt;\",\n  \"io.xskipper.parquet.mdlocation.type\" -&gt;  \"EXPLICIT_LOCATION\")\nxskipper.setParams(params)\n</code></pre> <p>   Note that when setting the type to <code>HIVE_TABLE_NAME</code> you should first set the parameter <code>io.xskipper.parquet.mdlocation</code> in the table properties to point to the metadata location.  </p> <p>You can the set the metadata location for hive table manually using the following DDL:     <pre><code>ALTER TABLE myDb.myTable\nSET TBLPROPERTIES ('io.xskipper.parquet.mdlocation'='/location/of/metadata')\n</code></pre></p> <p>If the location does not exist in the Hive table, xskipper will try to look up the parameter <code>io.xskipper.parquet.mdlocation</code> in the table's database and treat it as a base path location.</p> <p>When using <code>HIVE_TABLE_NAME</code> as the location type once indexing is done the parameter <code>io.xskipper.parquet.mdlocation</code> is set in the table properties.   (in case a fallback to the database base path occured then the resolved path from using the database metadata location as base path is stored in the table properties)</p>"},{"location":"api/configuration/parquet-mdstore-configuration/#metadata-path-resolving","title":"Metadata path resolving","text":"<p>The following explains how the metadata path is resolved:</p> <ul> <li> <p>During Indexing the metadata location is inferred according to the parameters parameters <code>io.xskipper.parquet.mdlocation</code> and <code>io.xskipper.parquet.mdlocation.type</code> which are set on the xskipper instance. These parameters are the default JVM wide parameters unless set differently for the given instance.</p> </li> <li> <p>During query run time, the metadata location is inferred according to the following</p> <ul> <li> <p>If there is an active xskipper instance in the JVM for the dataset/table use it's metadata configuration. Otherwise: </p> </li> <li> <p>For Datasets:</p> <ul> <li>Look up the parameters <code>io.xskipper.parquet.mdlocation</code> and <code>io.xskipper.parquet.mdlocation.type</code> in the JVM wide configuration in order to infer the metadata location.</li> </ul> </li> <li> <p>For Hive tables (with partitions<sup>1</sup>):</p> <ul> <li>If the table contains the parameter <code>io.xskipper.parquet.mdlocation</code> use it as the metadata location.</li> <li>If not, look up the parameter <code>io.xskipper.parquet.mdlocation</code> in the table's database and treat it as a base path location.</li> </ul> </li> </ul> </li> </ul> <ol> <li> <p>To index tables without partitions, index the physical location directly.\u00a0\u21a9</p> </li> </ol>"},{"location":"api/developer/build/","title":"Build","text":""},{"location":"api/developer/build/#building-from-source","title":"Building from source","text":"<p>Xskipper is compiled using SBT.</p> <p>To compile, run</p> <pre><code>build/sbt compile\n</code></pre> <p>To generate artifacts, run</p> <pre><code>build/sbt package\n</code></pre> <p>To execute tests, run</p> <pre><code>build/sbt test\n</code></pre> <p>Refer to SBT docs for more commands.</p>"},{"location":"api/developer/generate-doc/","title":"API Doc Generation","text":""},{"location":"api/developer/generate-doc/#api-doc-generation","title":"API Doc Generation","text":"<p>The following contains instructions on how to generate the Scala and Python docs for Xskipper.</p>"},{"location":"api/developer/generate-doc/#xskipper-python-docs","title":"Xskipper Python Docs","text":"<p>Xskipper Python docs are generated using Sphinx</p>"},{"location":"api/developer/generate-doc/#installation","title":"Installation","text":"<pre><code>pip install -U Sphinx\npip install mock\n</code></pre>"},{"location":"api/developer/generate-doc/#local-changes","title":"Local Changes","text":"<p>The doc configs are located in <code>python/doc</code>.</p> <p>Make the needed changes to the python files and then create the HTML files using:</p> <pre><code>cd python/doc\nmake html\n</code></pre> <p>The results will be located under <code>_build/html</code></p> <p>Note</p> <p>By default the version that will be used is the version that is written in<code>version.sbt</code> If you would like to change the version change it in the config file <code>conf.py</code></p>"},{"location":"api/developer/generate-doc/#publishing","title":"Publishing","text":"<p>To update the docs copy <code>_build/html</code> to <code>site/docs/api/pythondoc/&lt;version_number&gt;</code>. For example (assuming you shell is pointed at the project root):</p> <pre><code>cp -r python/doc/_build/html site/docs/api/pythondoc/1.3.0\n</code></pre> <p>Then update the index file to point to the new API documentation.</p>"},{"location":"api/developer/generate-doc/#xskipper-scala-docs","title":"Xskipper Scala Docs","text":"<p>Xskipper Scala docs are generated using sbt doc</p>"},{"location":"api/developer/generate-doc/#generate-the-docs","title":"Generate the docs","text":"<p>Run the following:</p> <pre><code>sbt clean compile package\nsbt doc\n</code></pre>"},{"location":"api/developer/generate-doc/#publishing_1","title":"Publishing","text":"<p>To update the docs copy <code>target/scala-2.12/api/</code> to <code>../scaladoc/1.3.0/</code>. For example (assuming you shell is pointed at the project root):</p> <pre><code>cp -r target/scala-2.12/api/ site/docs/api/scaladoc/1.3.0/\n</code></pre> <p>Then update the index file to point to the new API documentation.</p>"},{"location":"api/developer/metadata-versioning/","title":"Metadata versioning","text":""},{"location":"api/developer/metadata-versioning/#metadata-versioning","title":"Metadata Versioning","text":"<p>The versioning mechanism, version numbers and inter-version support matrix is up to the metadata store implementation. However, some details were pulled above the metadata store abstraction.  </p> <p>A metadata store must be able to report the metadata version status, which is defined as follows:</p> <p><pre><code>object MetadataVersionStatus extends Enumeration {\n  type MetadataVersionStatus = Value\n\n  /**\n    * The stored metadata is from exactly the same version\n    * as the metadata version of this jar.\n    */\n  val CURRENT = Value(\"current\")\n\n  /**\n    * The stored metadata is from a version strictly smaller than\n    * the metadata version of this jar. however, it can be used for filtering\n    * and a REFRESH operation will result in the metadata having the `CURRENT` status\n    */\n  val DEPRECATED_SUPPORTED = Value(\"deprecated_supported\")\n\n  /**\n    * The stored metadata is from a version strictly smaller than\n    * the metadata version of this jar, a version so old it can't be used for filtering.\n    * a REFRESH operation may or may not be able to upgrade it, up to the\n    * metadata store's `isMetadataUpgradePossible` method.\n    */\n  val DEPRECATED_UNSUPPORTED = Value(\"deprecated_unsupported\")\n\n  /**\n    * The stored metadata is from a version which is strictly greater\n    * than the metadata version of this jar.\n    * the metadata store is not expected to able to either read or refresh this metadata.\n    */\n  val TOO_NEW = Value(\"too_new\")\n}\n</code></pre> and so the metadata store must be able to perform the following operation regarding versioning:</p> <ol> <li>Report the Metadata Version Status:       <pre><code>def getMdVersionStatus(): MetadataVersionStatus\n</code></pre></li> <li>Report whether or not a metadata upgrade is possible:       <pre><code>def isMetadataUpgradePossible(): Boolean\n</code></pre></li> <li>Perform a Metadata Upgrade if reported as possible:       <pre><code>def upgradeMetadata(): Unit\n</code></pre></li> </ol> <p>During a REFRESH operation, the metadata version status is first checked, and upgraded if necessary, before performing the actual refresh or removing metadata for deleted objects. Any failure during this process (e.g., metadata version status <code>TOO_NEW</code>, <code>isMetadataUpgradePossible</code> returning false etc.) will fail the refresh.</p>"},{"location":"api/developer/parquet-metadatastore-spec/","title":"Parquet metadatastore spec","text":""},{"location":"api/developer/parquet-metadatastore-spec/#parquet-metadatastore-spec","title":"Parquet Metadatastore Spec","text":"<p>This is a specification for how metadata is represented in the Parquet Metadata store. We support multiple versions and document them here.  Note that the version number is stored in metadata files in a specific column of their Spark schema. Behind the scenes it is saved in the KV Metadata of the resulting Parquet files. This means that the version number can never imply metadata file locations, prefixes, directory layouts etc., since if one knows the version number, it means the metadata files have already been located.</p>"},{"location":"api/developer/parquet-metadatastore-spec/#how-to-maintain-the-version-numbers","title":"How to maintain the version numbers","text":"<p>The version numbers are natural numbers. Each release can change the version number by at most one. In particular, if 2 (or more) changes were made to the specification but no release happened between them, they will be considered as belonging to the same version.</p> <p>Note: </p> <ol> <li>Terminology - The terms \"KV Store\", \"KV Metadata\", \"Spark Schema Metadata\", despite having different meaning usually, will be used interchangeably to mean the structure Spark Schema Metadata, which in itself is assigned per-column. we will use them in the context of describing  what metadata for which column is laid out in what way.</li> <li>We will describe the structure of the spark schema, Spark's per-column      metadata, and use Spark types. this will allow us to be detached      from how spark actually represents these structures in the Parquet schema.</li> </ol>"},{"location":"api/developer/parquet-metadatastore-spec/#format-specifications","title":"Format Specifications","text":""},{"location":"api/developer/parquet-metadatastore-spec/#version-4","title":"Version 4","text":"<p>This version differs from Version 3 by:</p> <ol> <li>Saving the partition columns in the metadata for partitioned table in order to get the indexed files only for relevant partitions </li> <li>The partition columns are saved as <code>virtual_&lt;column_name&gt;</code></li> </ol>"},{"location":"api/developer/parquet-metadatastore-spec/#version-3","title":"Version 3","text":"<p>This version differs from Version 2 by:</p> <ol> <li>Configuration parameters prefix changed from <code>com.ibm.metaindex</code> to <code>io.xskipper</code></li> <li>The index parameters are now stored as a map</li> </ol>"},{"location":"api/developer/parquet-metadatastore-spec/#version-2","title":"Version 2","text":"<p>This version differs from Version 1 by:</p> <ol> <li> <p>Column name generation: For each index \\(I\\) (assume \\(I\\) is the name), defined on columns \\(c_1,...,c_n\\) (in this order), for each \\(c_i\\), let \\(c'_i\\) denote \\(c_i\\) with the following changes, in this order:</p> <ul> <li> <p>Replace all <code>#</code> with <code>##</code></p> </li> <li> <p>Replace all <code>.</code> with <code>$#$</code> The column name will be:  \\(c'_1c'_2...c'_n\\_I\\)len(c'_1)-len(c'_2)-...-len(c'_n)</p> </li> </ul> <p>That is, the transformed column names concatenated, followed by the index name,   followed by the lengths of the transformed column names concatenated with <code>-</code> as the delimiter  Example of such transformation:  <code>SomeIndex on \"lat#_.$_new\" and  \"$_lng.#\"</code> will get <code>lat##_$#$$_new_$_lng$#$##_someindex_14-10</code>  since <code>\"lat#_.$_new\"</code> will be mapped to <code>\"lat##_$#$$_new\"</code> with length 14, and <code>\"$_lng.#\"</code> will be mapped to <code>\"$_lng$#$##\"</code> with length 10</p> </li> </ol>"},{"location":"api/developer/parquet-metadatastore-spec/#version-1","title":"Version 1","text":"<p>This version differs from version 0 by:</p> <ul> <li>the addition of PME Support.</li> <li>changes to the way column names are constructed from indexes</li> <li>moving <code>tableIdentifier</code> metadata field to be under the <code>obj_name</code> column</li> <li>Min/Max index is saved as nested field with native parquet types</li> <li>Value List is saved as parquet array type</li> </ul> <p>Notes:</p> <ol> <li> <p>all additions were made in order for our library to be able to regenerate the encryption config (e.g., for refresh or during compaction).  these configs are not used by PME itself (PME uses other fields in the parquet file itself, these are not available to us via spark).  Theoretically it is possible to create a parquet file in which our metadata indicates an encryption config completely different than the one with which it's actually encrypted. we should avoid that.</p> </li> <li> <p>IMPORTANT no actual key material is ever written to the KV store, it's ALWAYS labels (e.g. <code>encryption.column.keys</code>)</p> </li> <li> <p>As of version 0 (and 1), the set of column names in spark schema for all indexes  is prefix free. this must remain the case, as the spark column names are used by our lib to  derive the set of columns in the parquet schema for a specific index (e.g., a <code>UDT</code> translated to a column  with a different name in the parquet schema).</p> </li> </ol> <p>Changes:</p> <ol> <li> <p>Additions to <code>obj_name</code> metadata:     If the metadata is not encrypted, then no additions are made.     If at least 1 index is encrypted, encryption metadata will be added the following way:</p> <ul> <li>key <code>encryption</code> of type <code>spark.sql.types.Metadata</code>, pointing to a metadata with with the following structure:</li> <li>key <code>encryption.column.keys</code> pointing to a String, containing the key list string for PME.    the format of this string matches the format for the config with the same name used in PME. see this</li> <li>optional key \"encryption.plaintext.footer\" of type String, containing one of <code>{true, false}</code>, indicating whether or not plaintext footer is used    if this key is not defined, then plaintext footer is implicitly disabled.</li> <li>key <code>encryption.footer.key</code> of type String, containing the footer key label (footer master Key ID in PME Terminology).    this key is also used to encrypt the footer (the footer is always encrypted if encryption is on)    this field is mandatory as a footer key is necessary if we use PME, even if plaintext    footer mode is in use (the footer key is used only for signing in this case, and of course for <code>obj_name</code>).</li> </ul> </li> <li> <p>Additions to each index metadata:     Indexes which are not encrypted remain unchanged.     For encrypted indexes, the following is added:</p> <ul> <li>key <code>key_metadata</code> of type String, pointing to the label of the key used to encrypt the set of columns for this index.  The set of columns for a specific is obtained by acquiring the Parquet schema tree, and taking all the paths to leaves which start with the Spark column name for this index (this is why the set of spark column names must be prefix free). for example, for a <code>MinMax</code> on <code>temp</code>, the set of columns we need to encrypt is <code>{temp_minmax.metadata}</code></li> </ul> <p>Note that this key label must be consistent with the one with which the columns for this index are encrypted, as configured in the column key list string in the <code>obj_name</code> metadata. if they are inconsistent, then this is a bug in the lib. the column key list string is kept in the <code>obj_name</code> metadata to save unnecessary scans to re-create that config when refreshing/compacting. the <code>key_metadata</code> in each index is used e.g. when listing existing indexes (to be able to retrieve the <code>keyMetadata</code> field in the <code>Index</code> case class).</p> </li> <li> <p>Column names for indexes are generated the same as in version 0, but the delimiter is now <code>_</code>     and not <code>:</code>, so for example, a <code>SomeIndex</code> over <code>a,b</code> would have gotten the column name <code>a:b_someindex</code> in version 0, now gets the column name <code>a_b_someindex</code></p> </li> <li> <p><code>tableIdentifier</code> metadata field is now saved only under the <code>obj_name</code> column (removed from index columns).</p> </li> <li> <p>Min/Max index is saved by having a nested field with <code>min</code> and <code>max</code> subfields each containing the value in native parquet type.</p> </li> <li> <p>Value List index is saved by saving an array data type</p> </li> </ol>"},{"location":"api/developer/parquet-metadatastore-spec/#version-0","title":"Version 0","text":"<p>The metadata is represented by one row for each object, with the object  name in its own column, and the metadata for each index in its own column as well, with the actual content of the metadata being the serialization of the UDT for this specific Metadata type.</p> <ol> <li><code>obj_name</code> column:     stores the object name.<ul> <li>For Unversioned files, defined as     <pre><code> StructField(\"obj_name\", StringType, false)\n</code></pre>      &gt; That is, a non-nullable column named \"obj_name\"              of type String, without metadata.</li> <li>For Versioned files (that is, version 0), defined as:     <pre><code>val objNameMeta =  new sql.types.MetadataBuilder()\n               .putLong(\"version\", 0)\n               .build()\n    StructField(\"obj_name\", StringType, false, objNameMeta)\n</code></pre>     &gt; That is, a non-nullable column named \"obj_name\"      of type String, with Metadata containing a single key, \"version\",       that points to a Long</li> </ul> </li> <li> <p>Per-Index Columns:</p> <p>For each index \\(I\\) (assume \\(I\\) is the name), defined on columns \\(c_1,...,c_n\\) (in this order), with UDT type T and params given as</p> <pre><code>params : Map[String, String] \n</code></pre> </li> </ol> <p>A column with the following properties is defined:</p> <ul> <li><code>name</code>: \\(c_1,...,c_n\\)_\\(I\\) <p>that is, the column names concatenated with a \":\" delimiter, followed by     the index name concatenated with <code>_</code></p> </li> <li><code>dataType</code>: <code>T</code> <p>That is, the UDT associated with this index.</p> </li> <li><code>nullable</code> - <code>true</code></li> <li><code>metadata</code>, a single key named <code>index</code>, pointing to another <code>spark.sql.types.Metadata</code>    with the following structure:<pre><code>* key `cols`, pointing to a `java.lang.String[]` containing the index columns.\n* key `name`, pointing to the String `I`\n* key `tableIdentifier`, pointing to String generated from the URI in the following manner:\n    - if the URI's Scheme is `COS`, then `&lt;bucket_name&gt;/&lt;object_name&gt;`\n    - else, if the path for this URI (obtained by `new URI(uri).getPath()`)\n     starts with a \"/\", the the preceding / is trimmed from this path, else it's unchanged.\n* Optional if `params` is not empty, then a key `params` points to `params`.\n</code></pre> </li> </ul>"},{"location":"api/developer/parquet-metadatastore-spec/#unversioned-files","title":"Unversioned Files","text":"<p>The Layout of the KV Store had several incarnations before it was versioned, so if looking at a metadata file (or group of files) without a version number,  we will implicitly treat them as version 0, which will act as  the \"as-built drawing\" for the KV Store layout, as of the time the version number was introduced. It's not defined what will happen should we encounter a file without a version, with KV Layout other than version 0.</p>"},{"location":"concepts/data-skipping/","title":"Data Skipping","text":""},{"location":"concepts/data-skipping/#data-skipping","title":"Data Skipping","text":"<p>Note</p> <p>This section explains the concepts that are use in Xskipper.  For usage details go to the Getting Started Page.</p> <p>Data skipping can significantly boost the performance of SQL queries by skipping over irrelevant data objects or files based on summary metadata associated with each object.</p> <p>For every column in the object, the summary metadata might include minimum and maximum values, a list or bloom filter of the appearing values, or other metadata which succinctly represents the data in that column. We call this metadata a data skipping index (or simply index), and it is used during query evaluation to skip over objects which have no relevant data.</p> <p>Xskipper supports all of Spark's native data formats, including Parquet, ORC, CSV, JSON and Avro. Data skipping is a performance optimization feature which means that using data skipping does not affect the content of query results.</p> <p>Xskipper can be used to easily define new data skipping index types using a concept we call Extensible Data Skipping.</p> <p>For more information about usage see:</p> <ul> <li>Quick Start Guide</li> <li>Example Notebooks</li> <li>API Reference</li> </ul>"},{"location":"concepts/extensible/","title":"Extensible Data Skipping","text":""},{"location":"concepts/extensible/#extensible-data-skipping","title":"Extensible Data Skipping","text":"<p>Read the IEEE Big Data 2020 paper - Extensible Data Skipping</p> <p>Xskipper creates a level of abstraction between index creation/query evaluation, and the actual metadata store implementation, which is unique to each metadata store.</p> <p>This abstraction operates in two main areas:</p> <ol> <li>Indexing Flow - abstract metadata is generated during index creation and refresh by analyzing a DataFrame.</li> </ol> <p></p> <ol> <li>Query Evaluation Flow - abstract metadata clauses are generated by filters that analyze the Catalyst (Spark optimizer) expression tree for pushdown predicates, and identify subtrees that can be mapped to a metadata Clause.</li> </ol> <p></p> <p>These abstract structures (MetaData and Clauses) are then translated to a representation that matches a specific metadata store. Metadata Clauses can then be applied to MetaDataTypes in an efficient manner.</p>"},{"location":"concepts/indexing-flow/","title":"Indexing Flow","text":""},{"location":"concepts/indexing-flow/#indexing-flow","title":"Indexing Flow","text":""},{"location":"concepts/indexing-flow/#definitions","title":"Definitions","text":""},{"location":"concepts/indexing-flow/#index","title":"Index","text":"<p>An Index is a collection of data skipping metadata. In general data skipping metadata can enable skipping row or column subsets of a dataset. In our case we skip data at object/file granularity.  </p>"},{"location":"concepts/indexing-flow/#metadata-generator","title":"Metadata Generator","text":"<p>A component which generates abstract metadata by processing an object/file.  The metadata is abstract in the sense that it's defined in memory. The concrete storage format for the metadata is implemented by the Metadata Translator.</p>"},{"location":"concepts/indexing-flow/#metadata-translator","title":"Metadata Translator","text":"<p>A component which translates the metadata created by the index to a suitable format in order to be stored in the metadatastore.</p>"},{"location":"concepts/indexing-flow/#index-creation-flow","title":"Index Creation Flow","text":"<p>Index creation runs in 2 phases:</p> <ol> <li> <p>Generaring abstract metadata types which hold the metadata in memory.</p> </li> <li> <p>Translating the abstract metadata types to a metadatastore representation and storing it.</p> </li> </ol> <p>Xskipper currently supports a parquet metadata store which stores the metadata in parquet files. For more information about the parquet metadata store see here.</p>"},{"location":"concepts/query-evaluation-flow/","title":"Query Evaluation Flow","text":""},{"location":"concepts/query-evaluation-flow/#query-evaluation-flow","title":"Query Evaluation Flow","text":""},{"location":"concepts/query-evaluation-flow/#definitions","title":"Definitions","text":""},{"location":"concepts/query-evaluation-flow/#clause","title":"Clause","text":"<p>We analyze Expression Trees and label tree nodes with Clauses.</p> <p>A Clause is a boolean condition that can be applied to a data subset (i.e, object) \\(S\\), typically by inspecting its metadata. For a Clause \\(c\\) and a (boolean) query expression \\(e\\), we say that \\(c\\) represents \\(e\\) (denoted by \\(c \\wr e\\) ), if for every object \\(S\\), whenever there exists a row \\(r \\in S\\) that satisfies \\(e\\), then  \\(S\\) satisfies \\(c\\). This means that if \\(S\\) does not satisfy \\(c\\), then \\(S\\) can be safely skipped when evaluating the query expression \\(e\\).</p> <p>For example, given the expression \\(e = temp &gt; 101\\) the clause \\(c = \\max_{r \\in S} temp(r) &gt; 101\\) represents \\(e\\) (denoted by \\(c \\wr e\\) ). Therefore, objects where \\(c = \\max_{r \\in S} temp(r) &lt;= 101\\) can be safely skipped</p>"},{"location":"concepts/query-evaluation-flow/#filter","title":"Filter","text":"<p>The labeling process of Expression Trees is done using filters. An algorithm A is a\ffilter if it performs the following action: When given an expression tree \\(e\\) as input, for every (boolean valued) vertex \\(v\\) in \\(e\\), it adds a set of clauses \\(C\\) s.t \\(\\forall c \\in C\\): \\(c \\wr v\\).</p> <p>For example, given the expression \\(e = temp &gt; 101\\):</p> <p></p> <p>A filter \\(f\\) might label the Expression Tree using a <code>MaxClause</code>:</p> <p></p> <p><code>MaxClause(c,&gt;,v)</code> is defined as \\(c = \\max_{r \\in S} c(r) &gt; v\\) Where for a <code>c</code> is the column name <code>v</code> is the value. Since <code>MaxClause(temperature,&gt;,101)</code> represents the node to which it was applied, \\(f\\) acted as a \ffilter.</p>"},{"location":"concepts/query-evaluation-flow/#clause-translator","title":"Clause Translator","text":"<p>A component which translates a Clause to a specific implementation according to the metadatastore type.</p>"},{"location":"concepts/query-evaluation-flow/#query-evaluation-flow_1","title":"Query Evaluation Flow","text":"<p>Query evaluation is done in 2 phases:</p> <ol> <li> <p>A query\u2019s Expression Tree \\(e\\) is labelled using a set of clauses</p> <ol> <li> <p>The clauses are combined to provide a single clause which represents \\(e\\).</p> </li> <li> <p>The labelling process is extensible, allowing for new index types and UDFs.</p> </li> </ol> </li> <li> <p>The clause is translated to a form that can be applied at the metadata store to filter out objects which can be skipped during query run time.</p> </li> </ol>"},{"location":"concepts/query-evaluation-flow/#a-simple-example","title":"A simple example","text":"<p>For example, given the query: <pre><code>SELECT *\nFROM employees\nWHERE salary &gt; 5 AND\nname IN (\u2018Danny\u2019, \u2019Moshe\u2019, \u2019Yossi\u2019)\n</code></pre></p> <p>The Expression Tree can be visualized as following:</p> <p></p> <p>Assuming we have a <code>MinMax</code> Index for the <code>salary</code> column (store minimum and maximum values for each object) and a <code>ValueList</code> Index on the <code>name</code> column (storing the distinct list of values for each object).</p> <ul> <li>Applying the <code>MinMax</code> filter results in:</li> </ul> <p></p> <ul> <li>Applying the <code>ValueList</code> filter on the results of the previous filter results in:</li> </ul> <p></p> <ul> <li>Finally we generate a combined Abstract Clause: <pre><code>AND(MaxClause(salary, &gt;, 5),ValueListClause(name, ('Danny', 'Moshe', 'Yossi')))\n</code></pre></li> </ul> <p>This clause will be translated to a form that can be applied at the metadata store to filter out objects which can be skipped during query run time</p>"},{"location":"getting-started/operatefirst-getting-started/","title":"Operatefirst getting started","text":""},{"location":"getting-started/operatefirst-getting-started/#data-skipping","title":"Data Skipping","text":"<p>Xskipper is enabled and supported in most spark notebook images and ready for you to use. If not present, update pyspark packages to include the relevant xskipper jar based on spark release. </p> <p>Data skipping can significantly boost the performance of SQL queries by skipping over irrelevant data objects or files based on summary metadata associated with each object.</p> <p>For every column in the object, the summary metadata might include minimum and maximum values, a list or bloom filter of the appearing values, or other metadata which succinctly represents the data in that column. We call this metadata a data skipping index (or simply index), and it is used during query evaluation to skip over objects which have no relevant data.</p> <p>Xskipper supports all of Spark's native data formats, including Parquet, ORC, CSV, JSON and Avro. Data skipping is a performance optimization feature which means that using data skipping does not affect the content of query results.</p> <p>Xskipper can be used to easily define new data skipping index types using a concept we call Extensible Data Skipping, more information can be found on xskipper site.</p> <p>For more information about usage see:</p> <ul> <li>API</li> <li>Example Notebooks</li> </ul>"},{"location":"getting-started/quick-start-guide/","title":"Quick Start Guide","text":""},{"location":"getting-started/quick-start-guide/#quick-start-guide","title":"Quick-Start Guide","text":"<p>This guide helps you quickly get started using Xskipper with Apache Spark.</p> <p>Note</p> <p>For advanced details see the API section and the full API Reference.  See here for sample notebooks.</p>"},{"location":"getting-started/quick-start-guide/#setup-apache-spark-with-xskipper","title":"Setup Apache Spark with Xskipper","text":"<p>Xskipper is compatible with Apache Spark 3.x, There are two ways to setup Xskipper:</p> <ol> <li>Run interactively: Start the Spark shell (Scala or Python) with Xskipper and explore Xskipper interactively.</li> <li>Run as a project: Set up a Maven or SBT project (Scala or Java) with Xskipper.</li> </ol>"},{"location":"getting-started/quick-start-guide/#run-with-an-interactive-shell","title":"Run with an interactive shell","text":"<p>To use Xskipper interactively within the Spark\u2019s Scala/Python shell, you need a local installation of Apache Spark. Follow the instructions here to install Apache Spark.</p>"},{"location":"getting-started/quick-start-guide/#spark-scala-shell","title":"Spark Scala Shell","text":"<p>Start a Spark Scala shell as follows:</p> <pre><code>./bin/spark-shell --packages io.xskipper:xskipper-core_2.12:1.5.0-SNAPSHOT\n</code></pre>"},{"location":"getting-started/quick-start-guide/#pyspark","title":"PySpark","text":"<p>Install or upgrade PySpark (3.2 or above) by running the following:</p> <pre><code>pip install --upgrade pyspark\n</code></pre> <p>Then, run PySpark with the Xskipper package:</p> <pre><code>pyspark --packages io.xskipper:xskipper-core_2.12:1.5.0-SNAPSHOT\n</code></pre>"},{"location":"getting-started/quick-start-guide/#run-as-a-project","title":"Run as a project","text":"<p>To build a project using the Xskipper binaries from the Maven Central Repository, use the following Maven coordinates:</p>"},{"location":"getting-started/quick-start-guide/#maven","title":"Maven","text":"<p>Include Xskipper in a Maven project by adding it as a dependency in the project's POM file. Xskipper should be compiled with Scala 2.12.</p> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;io.xskipper&lt;/groupId&gt;\n  &lt;artifactId&gt;xskipper-core_2.12&lt;/artifactId&gt;\n  &lt;version&gt;1.5.0-SNAPSHOT&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"getting-started/quick-start-guide/#sbt","title":"SBT","text":"<p>Include Xskipper in an SBT project by adding the following line to its build.sbt file:</p> <pre><code>libraryDependencies += \"io.xskipper\" %% \"xskipper-core\" % \"1.5.0-SNAPSHOT\"\n</code></pre>"},{"location":"getting-started/quick-start-guide/#python","title":"Python","text":"<p>To set up a Python project, first start the Spark session using the Xskipper package and then import the Python APIs.</p> <pre><code>spark = pyspark.sql.SparkSession.builder.appName(\"Xskipper\") \\\n    .config(\"spark.jars.packages\", \"io.xskipper:xskipper-core_2.12:1.5.0-SNAPSHOT\") \\\n    .getOrCreate()\n\nfrom xskipper import Xskipper\nfrom xskipper import Registration\n</code></pre>"},{"location":"getting-started/quick-start-guide/#configure-xskipper","title":"Configure Xskipper","text":"<p>In this example, we configure a JVM wide parameter to a base path which stores all data skipping indexes. The indexes can be stored on the same storage system as the data, but not under the same path. During query time indexes will be consulted at this location.</p> <p>For more configuration options, see configuration options.</p> PythonScala <pre><code>from xskipper import Xskipper\n\n# The base location to store all indexes \n# TODO: change to your index base location\nmd_base_location = \"/tmp/metadata\"\n\n# Configuring the JVM wide parameters\nconf = dict([\n            (\"io.xskipper.parquet.mdlocation\", md_base_location),\n            (\"io.xskipper.parquet.mdlocation.type\", \"EXPLICIT_BASE_PATH_LOCATION\"),\n            (\"io.xskipper.parquet.filter.dedup\", \"false\")])\nXskipper.setConf(spark, conf)\n</code></pre> <pre><code>import io.xskipper._\nimport io.xskipper.implicits._\n\n// The base location to store all indexes\n// TODO: change to your index base location\nval md_base_location = s\"/tmp/metadata\"\n\n// Configuring the JVM wide parameters\nval conf = Map(\n  \"io.xskipper.parquet.mdlocation\" -&gt; md_base_location,\n  \"io.xskipper.parquet.mdlocation.type\" -&gt; \"EXPLICIT_BASE_PATH_LOCATION\",\n  \"io.xskipper.parquet.filter.dedup\" -&gt; \"false\")\nXskipper.setConf(conf)\n</code></pre>"},{"location":"getting-started/quick-start-guide/#indexing-a-dataset","title":"Indexing a Dataset","text":""},{"location":"getting-started/quick-start-guide/#creating-a-sample-dataset","title":"Creating a Sample Dataset","text":"<p>First, let's create a sample dataset.</p> PythonScala <pre><code>from pyspark.sql.types import *\n\n# TODO: change to your data location\ndataset_location = \"/tmp/data\"\n\ndf_schema = StructType([StructField(\"dt\", StringType(), True), StructField(\"temp\", DoubleType(), True),\\\n                      StructField(\"city\", StringType(), True), StructField(\"vid\", StringType(), True)])\n\ndata = [(\"2017-07-07\", 20.0, \"Tel-Aviv\", \"a\"), (\"2017-07-08\", 30.0, \"Jerusalem\", \"b\")]\n\ndf = spark.createDataFrame(data, schema=df_schema)\n\n# use partitionBy to make sure we have two objects\ndf.write.partitionBy(\"dt\").mode(\"overwrite\").parquet(dataset_location)\n\n# read the dataset back from storage\nreader = spark.read.format(\"parquet\")\ndf = reader.load(dataset_location)\ndf.show(10, False)\n</code></pre> <pre><code>import org.apache.spark.sql.Row\nimport org.apache.spark.sql.types.{DoubleType, StringType, StructField, StructType}\n\n// TODO: change to your data location\nval dataset_location = s\"/tmp/data\"\n\nval schema = List(\n  StructField(\"dt\", StringType, true),\n  StructField(\"temp\", DoubleType, true),\n  StructField(\"city\", StringType, true),\n  StructField(\"vid\", StringType, true)\n)\n\nval data = Seq(\n  Row(\"2017-07-07\", 20.0, \"Tel-Aviv\", \"a\"),\n  Row(\"2017-07-08\", 30.0, \"Jerusalem\", \"b\")\n)\n\nval ds = spark.createDataFrame(\n  spark.sparkContext.parallelize(data),\n  StructType(schema)\n)\n\n// use partitionBy to make sure we have two objects\nds.write.partitionBy(\"dt\").mode(\"overwrite\").parquet(dataset_location)\n\n// read the dataset back from storage\nval reader = spark.read.format(\"parquet\")\nval df = reader.load(dataset_location)\ndf.show(false)\n</code></pre>"},{"location":"getting-started/quick-start-guide/#indexing","title":"Indexing","text":"<p>When creating a data skipping index on a data set, first decide which columns to index, then choose an index type for each column. These choices are workload and data dependent. Typically, choose columns to which predicates are applied in many queries.</p> <p>The following index types are supported out of the box:</p> Index type Description Applicable to predicates in WHERE clauses Column types MinMax Stores minimum and maximum values for a column &lt;,&lt;=,=,&gt;=,&gt; All types except for complex types. See Supported Spark SQL data types. ValueList Stores the list of unique values for the column =,IN,LIKE All types except for complex types. See Supported Spark SQL data types. BloomFilter Uses a bloom filter to test for set membership =,IN Byte, String, Long, Integer, Short <p>MinMax results in small index size and is a good usually choice when the dataset's sort order is correlated with a given column. For the other 2 options,  choose value list if the number of distinct values in an object is typically much smaller than the total number of values in that object. On the other hand Bloom filters are recommended for columns with high cardinality (otherwise the index can get as big as that column of the data set).</p> <p>Note that Xskipper also enables to create your own data skipping indexes and specify how to use them during query time. For more details see here.</p> PythonScala <pre><code># create Xskipper instance for the sample dataset\nxskipper = Xskipper(spark, dataset_location)\n\n# remove index if exists\nif xskipper.isIndexed():\n    xskipper.dropIndex()\n\nxskipper.indexBuilder() \\\n        .addMinMaxIndex(\"temp\") \\\n        .addValueListIndex(\"city\") \\\n        .addBloomFilterIndex(\"vid\") \\\n        .build(reader) \\\n        .show(10, False)\n</code></pre> <pre><code>// create Xskipper instance for the sample dataset\nval xskipper = new Xskipper(spark, dataset_location)\n\n// remove existing index if needed\nif (xskipper.isIndexed()) {\n  xskipper.dropIndex()\n}\n\nxskipper.indexBuilder()\n        .addMinMaxIndex(\"temp\")\n        .addValueListIndex(\"city\")\n        .addBloomFilterIndex(\"vid\")\n        .build(reader)\n        .show(false)\n</code></pre>"},{"location":"getting-started/quick-start-guide/#viewing-index-status","title":"Viewing index status","text":"<p>The following code shows how a user can view the current index status to check which indexes exist on the dataset and whether the index is up-to-date.</p> PythonScala <pre><code>xskipper.describeIndex(reader).show(10, False)\n</code></pre> <pre><code>xskipper.describeIndex(reader).show(false)\n</code></pre>"},{"location":"getting-started/quick-start-guide/#list-indexed-datasets","title":"List Indexed datasets","text":"<p>The following code shows how a user can view all indexed datasets under the current base location.</p> PythonScala <pre><code>Xskipper.listIndexes(spark).show(10, False)\n</code></pre> <pre><code>Xskipper.listIndexes(spark).show(false)\n</code></pre>"},{"location":"getting-started/quick-start-guide/#using-data-skipping-indexes","title":"Using Data Skipping Indexes","text":""},{"location":"getting-started/quick-start-guide/#enabledisable-xskipper","title":"Enable/Disable Xskipper","text":"<p>Xskipper provides APIs to enable or disable index usage with Spark. By using the \"enable\" command, Xskipper optimization rules become visible to the Apache Spark optimizer and will be used in query optimization and execution. By using the \"disable' command, Xskipper optimization rules no longer apply during query optimization. Note that disabling Xskipper has no impact on created indexes, and they remain intact.</p> PythonScala <pre><code># Enable Xskipper\nXskipper.enable(spark)\n\n# Disable Xskipper\nXskipper.disable(spark)\n\n# You can use the following to check whether the Xskipper is enabled\nif not Xskipper.isEnabled(spark):\n    Xskipper.enable(spark)\n</code></pre> <pre><code>// Enable Xskipper\nspark.enableXskipper()\n\n// Disable Xskipper\nspark.disableXskipper()\n\n// You can use the following to check whether the Xskipper is enabled\nif (!spark.isXskipperEnabled()) {\n    spark.enableXskipper()\n}\n</code></pre> <p>Xskipper also provides an API to inject the <code>DataSkippingFileIndexRule</code> into catalyst as part of the operatorOptimization rules and enable it using spark session extensions injectOptimizerRule. By adding <code>io.xskipper.utils.RuleExtension</code> to spark.sql.extensions Xskipper optimization rules become visible to the Apache Spark optimizer and will be used in query optimization and execution.</p> <p>To use with thrift server, start the thrift server with the extension:     <pre><code>start-thriftserver.sh --jars &lt;XskipperJar&gt;\n                      --conf spark.sql.extensions=io.xskipper.RuleExtension\n                      --conf spark.sql.extensions=io.xskipper.utils.RuleExtension\nAlternatively, instead of --jars use --packages &lt;io.xskipper:xskipper-core release&gt;\n</code></pre></p>"},{"location":"getting-started/quick-start-guide/#running-queries","title":"Running Queries","text":"<p>Once Xskipper has been enabled you can run queries (using either SQL or the DataFrame API) and enjoy the performance and cost benefits of data skipping. There will be no change to query results. </p> <p>First, let's create a temporary view:</p> PythonScala <pre><code>df = reader.load(dataset_location)\ndf.createOrReplaceTempView(\"sample\")\n</code></pre> <pre><code>df.createOrReplaceTempView(\"sample\")\n</code></pre>"},{"location":"getting-started/quick-start-guide/#example-query-using-the-minmax-index","title":"Example query using the MinMax index","text":"PythonScala <pre><code>spark.sql(\"select * from sample where temp &lt; 30\").show()\n</code></pre> <pre><code>spark.sql(\"select * from sample where temp &lt; 30\").show()\n</code></pre>"},{"location":"getting-started/quick-start-guide/#inspecting-query-skipping-stats","title":"Inspecting query skipping stats","text":"<p>Note</p> <p>Starting from version 1.3.0, skipping stats are disabled for queries involving Data Source V2 file sources. note that after processing a query that involves Data Source v2 file sources, stats are disabled for all subsequent queries until the stats are cleared. See this issue</p> PythonScala <pre><code>Xskipper.getLatestQueryAggregatedStats(spark).show(10, False)\n</code></pre> <pre><code>Xskipper.getLatestQueryAggregatedStats(spark).show(false)\n</code></pre> <p>Note: the above returns the accumulated data skipping statistics for all of the datasets which were involved in the query. If you want to inspect the stats for a specific dataset you can call the API below to get stats on the Xskipper instance:</p> PythonScala <pre><code>xskipper.getLatestQueryStats().show(10, False)\n</code></pre> <pre><code>xskipper.getLatestQueryStats().show(false)\n</code></pre> <p>For more examples see the sample notebooks</p>"},{"location":"getting-started/quick-start-guide/#index-life-cycle","title":"Index Life Cycle","text":"<p>The following operations can be used in order to maintain the index.</p>"},{"location":"getting-started/quick-start-guide/#refresh-index","title":"Refresh Index","text":"<p>Over time the index can become stale as new files are added/removed/modified from the dataset. In order to bring the index up-to-date you can call the refresh operation which will index the new/modified files and remove obsolete metadata. Note: The index will still be beneficial for files which didn't change since the last indexing time even without refreshing.</p> <p>First let's simulate addition of new data to the dataset:</p> PythonScala <pre><code># adding new file to the dataset to simulate changes in the dataset\nupdate_data = [(\"2017-07-09\", 25.0, \"Beer-Sheva\", \"c\")]\n\nupdate_df = spark.createDataFrame(update_data, schema=df_schema)\n\n# append to the existing dataset\nupdate_df.write.partitionBy(\"dt\").mode(\"append\").parquet(dataset_location)\n</code></pre> <pre><code>val update_data = Seq(\n  Row(\"2017-07-09\", 25.0, \"Beer-Sheva\", \"c\")\n)\n\nval update_ds = spark.createDataFrame(\n  spark.sparkContext.parallelize(update_data),\n  StructType(schema)\n)\n\n// append to the existing dataset\nupdate_ds.write.partitionBy(\"dt\").mode(\"append\").parquet(dataset_location)\n</code></pre> <p>Now, let's inspect the index status:</p> PythonScala <pre><code>xskipper.describeIndex(reader).show(10, False)\n</code></pre> <pre><code>xskipper.describeIndex(reader).show(false)\n</code></pre> <p>In this case the index status will indicate that there are new files that are not indexed. Therefore we use the Refresh operation to update the metadata:</p> PythonScala <pre><code>xskipper.refreshIndex(reader).show(10, False)\n</code></pre> <pre><code>xskipper.refreshIndex(reader).show(false)\n</code></pre> <p>Now you can run the <code>describe</code> operation again and see that the metadata is up to date.</p>"},{"location":"getting-started/quick-start-guide/#drop-index","title":"Drop Index","text":"<p>In order to drop the index use the following API call:</p> PythonScala <pre><code>xskipper.dropIndex()\n</code></pre> <pre><code>xskipper.dropIndex()\n</code></pre>"},{"location":"getting-started/quick-start-guide/#working-with-hive-tables","title":"Working with Hive tables","text":"<p>Xskipper also supports skipping over hive tables. Note that indexing is for Hive tables with partitions. To index tables without partitions, index the physical location directly.</p> <p>The API for working with hive tables is similar to the API presented above with 2 main differences:</p> <ol> <li> <p>The uri used in the Xskipper constructor is the table identifier with the form: <code>&lt;db&gt;.&lt;table&gt;</code>.</p> </li> <li> <p>The API calls do not require a DataFrameReader.</p> </li> </ol> <p>For more information regarding the API see here.</p> <p>The index location for a hive table is resolved according to the following:</p> <ul> <li>If the table contains the parameter <code>io.xskipper.parquet.mdlocation</code> this value will be used as the index location.</li> <li>Otherwise, xskipper will look up the parameter <code>io.xskipper.parquet.mdlocation</code> in the table's database and will use it as the base index location for all tables.</li> </ul> <p>Note: During indexing, the index location parameter can be automatically added to the table properties if the xskipper instance is configured accordingly. For more info regarding the index location configuration see here.</p>"},{"location":"getting-started/quick-start-guide/#setting-the-base-index-location-in-the-database","title":"Setting the base index location in the database","text":"<p>In this example we will set the base location in the database.</p> PythonScala <pre><code>alter_db_ddl = (\"ALTER DATABASE default SET DBPROPERTIES ('io.xskipper.parquet.mdlocation'='{0}')\").format(md_base_location)\nspark.sql(alter_db_ddl)\n</code></pre> <pre><code>val alter_db_ddl = s\"ALTER DATABASE default SET DBPROPERTIES ('io.xskipper.parquet.mdlocation'='${md_base_location}')\"\nspark.sql(alter_db_ddl)\n</code></pre>"},{"location":"getting-started/quick-start-guide/#creating-a-sample-hive-table","title":"Creating a Sample Hive Table","text":"<p>Let's create a hive table on the dataset we created earlier:</p> PythonScala <pre><code>create_table_ddl = \"\"\"CREATE TABLE IF NOT EXISTS tbl ( \\\ntemp Double,\ncity String,\nvid String,\ndt String\n)\nUSING PARQUET\nPARTITIONED BY (dt)\nLOCATION '{0}'\"\"\".format(dataset_location)\nspark.sql(create_table_ddl)\n\n# recover the partitions\nspark.sql(\"ALTER TABLE tbl RECOVER PARTITIONS\")\n\n# verify the table was created\nspark.sql(\"show tables\").show(10, False)\nspark.sql(\"show partitions tbl\").show(10, False)\n</code></pre> <pre><code>val create_table_ddl =\n      s\"\"\"CREATE TABLE IF NOT EXISTS tbl (\n         |temp Double,\n         |city String,\n         |vid String,\n         |dt String\n         |)\n         |USING PARQUET\n         |PARTITIONED BY (dt)\n         |LOCATION '${dataset_location}'\n         |\"\"\".stripMargin\nspark.sql(create_table_ddl)\n\n// Recover the table partitions\nspark.sql(\"ALTER TABLE tbl RECOVER PARTITIONS\")\n\n// verify the table was created\nspark.sql(\"show tables\").show(false)\nspark.sql(\"show partitions tbl\").show(false)\n</code></pre>"},{"location":"getting-started/quick-start-guide/#indexing-a-hive-table","title":"Indexing a Hive Table","text":"<p>Note we use default.sample as the uri in the Xskipper constructor.</p> PythonScala <pre><code># create an Xskipper instance for the sample Hive Table\nxskipper_hive = Xskipper(spark, 'default.tbl')\n\n# remove index if exists\nif xskipper_hive.isIndexed():\n    xskipper_hive.dropIndex()\n\nxskipper_hive.indexBuilder() \\\n        .addMinMaxIndex(\"temp\") \\\n        .addValueListIndex(\"city\") \\\n        .addBloomFilterIndex(\"vid\") \\\n        .build() \\\n        .show(10, False)\n</code></pre> <pre><code>// create an Xskipper instance for the sample Hive Table\nval xskipper_hive = new Xskipper(spark, \"default.tbl\")\n\n// remove existing index if needed\nif (xskipper_hive.isIndexed()) {\n  xskipper_hive.dropIndex()\n}\n\nxskipper_hive.indexBuilder()\n        .addMinMaxIndex(\"temp\")\n        .addValueListIndex(\"city\")\n        .addBloomFilterIndex(\"vid\")\n        .build()\n        .show(false)\n</code></pre>"},{"location":"getting-started/quick-start-guide/#running-queries_1","title":"Running Queries","text":"<p>Once Xskipper has been enabled you can continue running queries (using either SQL or DataFrame API) and enjoy the benefits of data skipping.</p> <p>First, let's make sure Xskipper is enabled:</p> PythonScala <pre><code># You can use the following to check whether the Xskipper is enabled\nif not Xskipper.isEnabled(spark):\n    Xskipper.enable(spark)\n</code></pre> <pre><code>// You can use the following to check whether the Xskipper is enabled\nif (!spark.isXskipperEnabled()) {\n    spark.enableXskipper()\n}\n</code></pre>"},{"location":"getting-started/quick-start-guide/#example-query-using-the-minmax-index_1","title":"Example query using the MinMax index","text":"PythonScala <pre><code>spark.sql(\"select * from tbl where temp &lt; 30\").show(false)\n</code></pre> <pre><code>spark.sql(\"select * from tbl where temp &lt; 30\").show(false)\n</code></pre> <p>Inspecting the query stats:</p> PythonScala <pre><code>Xskipper.getLatestQueryAggregatedStats(spark).show(10, False)\n</code></pre> <pre><code>Xskipper.getLatestQueryAggregatedStats(spark).show(false)\n</code></pre>"},{"location":"getting-started/quick-start-guide/#index-life-cycle-hive-tables","title":"Index Life Cycle - Hive Tables","text":"<p>The API is similar to the dataset API but without the need for a <code>reader</code> instance.</p>"},{"location":"getting-started/quick-start-guide/#view-the-index-status","title":"View the index status","text":"PythonScala <pre><code>xskipper_hive.describeIndex().show(10, False)\n</code></pre> <pre><code>xskipper_hive.describeIndex().show(false)\n</code></pre>"},{"location":"getting-started/quick-start-guide/#refresh-index_1","title":"Refresh Index","text":"PythonScala <pre><code>xskipper_hive.refreshIndex().show(10, False)\n</code></pre> <pre><code>xskipper.refreshIndex(reader).show(false)\n</code></pre>"},{"location":"getting-started/quick-start-guide/#drop-index_1","title":"Drop Index","text":"<p>In order to drop the index use the following API call:</p> PythonScala <pre><code>xskipper_hive.dropIndex()\n</code></pre> <pre><code>xskipper_hive.dropIndex()\n</code></pre>"},{"location":"getting-started/sample-notebooks/","title":"Demo Notebooks","text":""},{"location":"getting-started/sample-notebooks/#demo-notebooks","title":"Demo Notebooks","text":"<p>You can use the following notebooks in order to get Started quickly with Xskipper:</p> <ul> <li>Python Notebook</li> <li>Scala Notebook</li> </ul>"}]}